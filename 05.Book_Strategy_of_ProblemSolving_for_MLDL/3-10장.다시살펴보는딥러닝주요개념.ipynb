{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 3부. 딥러닝 문제해결\n",
    "##### 10장. 다시 살펴보는 딥러닝 주요 개념\n",
    "- 10.1 인공 신경망\n",
    "- 10.2 합성곱 신경망(CNN)\n",
    "- 10.3 성능 향상을 위한 딥러닝 알고리즘\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 퍼셉트론은 입력값과 가중치의 선형결합 구조. 따라서 선형 분류문제만 풀 수 있다. \n",
    "2. 비선형 분류를 위해서는 퍼셉트론을 여러 층으로 쌓아야 한다.(multi-layer-perceptron) --> 인공신경망의 기본 구조 \n",
    "3. 활성화 함수에는 Sigmoid, ReLU, Leaky ReLU 가 대표적이다. \n",
    "   - Leaky ReLU는 ReLU를 변형한 것으로 입력값이 음수일때 무조건 0을 출력하지 않고 약간의 음수값이 계속 커지는 구조임 \n",
    "   - ![Activation Functions](images/activation_functions.png)\n",
    "\n",
    "4. 대표적인 손실함수(loss function)으로는, 회귀문제에 사용되는 MSE와 분류문제에 사용되는 cross entropy가 있다. \n",
    "5. 순전파(forward propagation)은 입력값과 가중치를 활용해 출력값(타겟 예측값)을 구하는 과정이다. \n",
    "6. 역전파(back propagation)은 손실값을 통해 기울기를 구해 가중치를 갱신하는 과정이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 필터는 입력값에서 특정한 특성을 필터링 즉 부각시키는 기능을 한다. (입력값의 요약)\n",
    "   - 입력값과 필터를 행렬곱한 값이 결과값 즉 특성맵이 된다. 따라서 입력값이 필터와 유사할 수록 결과값은 큰 값을 갖게됨 \n",
    "8. 풀링은 특성맵 크기를 줄여 이미지 요약정보를 추출한다. 이때 특정영역의 최대값 또는 평균값을 가져온다. \n",
    "   - 입력값을 필터를 통해 요약한 특성맵을 만들고, 여기에 풀링을 적용하여 더 요약하는 순서로 진행된다. \n",
    "9. 필터는 입력값에서 특정한 특성을 필터링 즉 부각시키는 기능을 한다. (입력값의 요약)\n",
    "   - 입력값과 필터를 행렬곱한 값이 결과값 즉 특성맵이 된다. 따라서 입력값이 필터와 유사할 수록 결과값은 큰 값을 갖게됨 \n",
    "10. 풀링은 특성맵 크기를 줄여 이미지 요약정보를 추출한다. 이때 특정영역의 최대값 또는 평균값을 가져온다. \n",
    "    - 입력값을 필터를 통해 요약한 특성맵을 만들고, 여기에 풀링을 적용하여 더 요약하는 순서로 진행된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "##### >> 이미지 처리 흐름\n",
    "- 필터는 이미지에서 에지, 코너, 패턴 등 중요한 특징을 감지합니다.\n",
    "- 풀링은 감지된 특징을 축소하여 데이터 크기를 줄이고, 중요 정보만 유지합니다.\n",
    "- 이러한 과정을 반복하여 고수준의 추상적 특징을 학습합니다.\n",
    "- 결론적으로, 필터는 \"무엇을 볼 것인지\"를 결정하고, 풀링은 \"정보를 간소화하여 저장\"하는 역할을 합니다.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. 배치정규화(batch normalization)은 오버피팅 방지와 훈련속도 향상을 위한 기법\n",
    "    - 신경망의 계층마다 입력데이터 분포가 다른 경우 훈련속도가 느려짐. 정규화와 스케일 조정을 통해 각 층의 입력데이터 분포를 유사하게 만든다. \n",
    "12. 옵티마이저는 신경망의 최적 가중치를 찾아주는 방법이다. \n",
    "    - 확률적 경사하강법(SGC)을 기본으로 많은 개량형이 있으나, 최근에는 Adam이 가장 많이 쓰인다. \n",
    "    - Adam은 모멘텀(SGD에 관성개념을 추가하여 속도증가)과 RMSProp(최적 파라미터에 도달할 수록 학습률을 낮추되, 훈련이 계속되어도 학습률이 0이 되지는 않도록 함)의 장점을 적용\n",
    "13. 전이학습(transfer learning)은 다른 영역에서 사전 훈련된 모델을 약간의 추가학습을 더해 사용하는 것. 보다 적은 데이터로 높은 성능을 얻을 수 있다. \n",
    "    - 사전 훈련된 모델의 신경망 전체를 재훈련 하는 경우(fine tuning), 다른 파라미터는 고정하고 일부 계층만 재훈련 하는 경우가 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
