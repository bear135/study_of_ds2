{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### model 2 : LightGBM with feature engineering & Hyperparameter tunning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(595212, 58) (892816, 57) (892816, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"pastel\", \n",
    "              rc={\"font.size\": 9, \"axes.titlesize\": 9, \"axes.labelsize\": 9,\n",
    "              \"xtick.labelsize\": 9, \"ytick.labelsize\": 9})  \n",
    "\n",
    "#data loading\n",
    "data_path = 'kaggle/input/porto-seguro-safe-driver-prediction/'\n",
    "train = pd.read_csv(data_path + 'train.csv', index_col = 'id')\n",
    "test = pd.read_csv(data_path + 'test.csv', index_col = 'id')\n",
    "submission = pd.read_csv(data_path + 'sample_submission.csv', index_col = 'id')\n",
    "\n",
    "print(train.shape, test.shape, submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1488028, 57)\n",
      "Index(['ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03', 'ps_ind_04_cat',\n",
      "       'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin',\n",
      "       'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin',\n",
      "       'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15', 'ps_ind_16_bin',\n",
      "       'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03',\n",
      "       'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat',\n",
      "       'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat',\n",
      "       'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat', 'ps_car_11',\n",
      "       'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15', 'ps_calc_01',\n",
      "       'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06',\n",
      "       'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11',\n",
      "       'ps_calc_12', 'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin',\n",
      "       'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin',\n",
      "       'ps_calc_20_bin'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#인코딩을 위한 데이터 합치기 \n",
    "all_data = pd.concat([train, test], ignore_index=True)\n",
    "all_data = all_data.drop('target', axis=1)\n",
    "print(all_data.shape)\n",
    "\n",
    "#전체 변수리스트 출력 \n",
    "all_features = all_data.columns \n",
    "print(all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Part 1. feature engineering \n",
    "   1. 명목변수 one-hot 인코딩 \n",
    "   2. 결측값의 갯수를 하나의 변수로 생성하여 추가 \n",
    "   3. 명목형과 cat형 변수를 제외한 remaining features 정의 \n",
    "   4. 명목형 변수들(cat, mix_ind)의 모든 고유값별 갯수를, 새로운 파생변수로 변환 \n",
    "   5. 위 1~4에서 생성된 변수들을 결합한 데이터셋 생성 \n",
    "   6. 데이터셋을 다시 train/test 데이터로 분리 \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat', 'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat', 'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1488028x184 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 20832392 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nominal features --> one-hot encoding \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_features = [feature for feature in all_features if 'cat' in feature]\n",
    "print(cat_features) \n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "encoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features])\n",
    "encoded_cat_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ps_ind_01</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_03</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_11_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_12_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_13_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_15</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_16_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_17_bin</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_ind_18_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_reg_01</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_reg_02</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_reg_03</th>\n",
       "      <td>0.718070</td>\n",
       "      <td>0.766078</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.580948</td>\n",
       "      <td>0.840759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_01_cat</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_02_cat</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_03_cat</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_04_cat</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_05_cat</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_06_cat</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_07_cat</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_08_cat</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_09_cat</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_10_cat</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_11_cat</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>82.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_11</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_12</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.374166</td>\n",
       "      <td>0.316070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_13</th>\n",
       "      <td>0.883679</td>\n",
       "      <td>0.618817</td>\n",
       "      <td>0.641586</td>\n",
       "      <td>0.542949</td>\n",
       "      <td>0.565832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_14</th>\n",
       "      <td>0.370810</td>\n",
       "      <td>0.388716</td>\n",
       "      <td>0.347275</td>\n",
       "      <td>0.294958</td>\n",
       "      <td>0.365103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_car_15</th>\n",
       "      <td>3.605551</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_01</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_02</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_03</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_04</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_05</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_06</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_07</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_08</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_09</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_10</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_11</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_13</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_14</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0          1          2           3          4\n",
       "ps_ind_01        2.000000   1.000000   5.000000    0.000000   0.000000\n",
       "ps_ind_02_cat    2.000000   1.000000   4.000000    1.000000   2.000000\n",
       "ps_ind_03        5.000000   7.000000   9.000000    2.000000   0.000000\n",
       "ps_ind_04_cat    1.000000   0.000000   1.000000    0.000000   1.000000\n",
       "ps_ind_05_cat    0.000000   0.000000   0.000000    0.000000   0.000000\n",
       "ps_ind_06_bin    0.000000   0.000000   0.000000    1.000000   1.000000\n",
       "ps_ind_07_bin    1.000000   0.000000   0.000000    0.000000   0.000000\n",
       "ps_ind_08_bin    0.000000   1.000000   1.000000    0.000000   0.000000\n",
       "ps_ind_09_bin    0.000000   0.000000   0.000000    0.000000   0.000000\n",
       "ps_ind_10_bin    0.000000   0.000000   0.000000    0.000000   0.000000\n",
       "ps_ind_11_bin    0.000000   0.000000   0.000000    0.000000   0.000000\n",
       "ps_ind_12_bin    0.000000   0.000000   0.000000    0.000000   0.000000\n",
       "ps_ind_13_bin    0.000000   0.000000   0.000000    0.000000   0.000000\n",
       "ps_ind_14        0.000000   0.000000   0.000000    0.000000   0.000000\n",
       "ps_ind_15       11.000000   3.000000  12.000000    8.000000   9.000000\n",
       "ps_ind_16_bin    0.000000   0.000000   1.000000    1.000000   1.000000\n",
       "ps_ind_17_bin    1.000000   0.000000   0.000000    0.000000   0.000000\n",
       "ps_ind_18_bin    0.000000   1.000000   0.000000    0.000000   0.000000\n",
       "ps_reg_01        0.700000   0.800000   0.000000    0.900000   0.700000\n",
       "ps_reg_02        0.200000   0.400000   0.000000    0.200000   0.600000\n",
       "ps_reg_03        0.718070   0.766078  -1.000000    0.580948   0.840759\n",
       "ps_car_01_cat   10.000000  11.000000   7.000000    7.000000  11.000000\n",
       "ps_car_02_cat    1.000000   1.000000   1.000000    1.000000   1.000000\n",
       "ps_car_03_cat   -1.000000  -1.000000  -1.000000    0.000000  -1.000000\n",
       "ps_car_04_cat    0.000000   0.000000   0.000000    0.000000   0.000000\n",
       "ps_car_05_cat    1.000000  -1.000000  -1.000000    1.000000  -1.000000\n",
       "ps_car_06_cat    4.000000  11.000000  14.000000   11.000000  14.000000\n",
       "ps_car_07_cat    1.000000   1.000000   1.000000    1.000000   1.000000\n",
       "ps_car_08_cat    0.000000   1.000000   1.000000    1.000000   1.000000\n",
       "ps_car_09_cat    0.000000   2.000000   2.000000    3.000000   2.000000\n",
       "ps_car_10_cat    1.000000   1.000000   1.000000    1.000000   1.000000\n",
       "ps_car_11_cat   12.000000  19.000000  60.000000  104.000000  82.000000\n",
       "ps_car_11        2.000000   3.000000   1.000000    1.000000   3.000000\n",
       "ps_car_12        0.400000   0.316228   0.316228    0.374166   0.316070\n",
       "ps_car_13        0.883679   0.618817   0.641586    0.542949   0.565832\n",
       "ps_car_14        0.370810   0.388716   0.347275    0.294958   0.365103\n",
       "ps_car_15        3.605551   2.449490   3.316625    2.000000   2.000000\n",
       "ps_calc_01       0.600000   0.300000   0.500000    0.600000   0.400000\n",
       "ps_calc_02       0.500000   0.100000   0.700000    0.900000   0.600000\n",
       "ps_calc_03       0.200000   0.300000   0.100000    0.100000   0.000000\n",
       "ps_calc_04       3.000000   2.000000   2.000000    2.000000   2.000000\n",
       "ps_calc_05       1.000000   1.000000   2.000000    4.000000   2.000000\n",
       "ps_calc_06      10.000000   9.000000   9.000000    7.000000   6.000000\n",
       "ps_calc_07       1.000000   5.000000   1.000000    1.000000   3.000000\n",
       "ps_calc_08      10.000000   8.000000   8.000000    8.000000  10.000000\n",
       "ps_calc_09       1.000000   1.000000   2.000000    4.000000   2.000000\n",
       "ps_calc_10       5.000000   7.000000   7.000000    2.000000  12.000000\n",
       "ps_calc_11       9.000000   3.000000   4.000000    2.000000   3.000000\n",
       "ps_calc_12       1.000000   1.000000   2.000000    2.000000   1.000000\n",
       "ps_calc_13       5.000000   1.000000   7.000000    4.000000   1.000000\n",
       "ps_calc_14       8.000000   9.000000   7.000000    9.000000   3.000000\n",
       "ps_calc_15_bin   0.000000   0.000000   0.000000    0.000000   0.000000\n",
       "ps_calc_16_bin   1.000000   1.000000   1.000000    0.000000   0.000000\n",
       "ps_calc_17_bin   1.000000   1.000000   1.000000    0.000000   0.000000\n",
       "ps_calc_18_bin   0.000000   0.000000   0.000000    0.000000   1.000000\n",
       "ps_calc_19_bin   0.000000   1.000000   1.000000    0.000000   1.000000\n",
       "ps_calc_20_bin   1.000000   0.000000   0.000000    0.000000   0.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#결측값 갯수를 파생변수로 \n",
    "all_data['num_missing'] = (all_data == -1).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ps_ind_01', 'ps_ind_03', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15', 'ps_ind_16_bin', 'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_11', 'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15', 'num_missing']\n"
     ]
    }
   ],
   "source": [
    "#remaining_features 정의 : nominal, calc이 아닌 변수\n",
    "remaining_features = [feature for feature in all_features\n",
    "                      if ('cat' not in feature and 'calc' not in feature) ]\n",
    "\n",
    "#결측값 파생변수(num_minning) 추가\n",
    "remaining_features.append('num_missing')\n",
    "\n",
    "print(remaining_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15', 'ps_ind_16_bin', 'ps_ind_17_bin', 'ps_ind_18_bin']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0          2_2_5_1_0_0_1_0_0_0_0_0_0_0_11_0_1_0_\n",
       "1           1_1_7_0_0_0_0_1_0_0_0_0_0_0_3_0_0_1_\n",
       "2          5_4_9_1_0_0_0_1_0_0_0_0_0_0_12_1_0_0_\n",
       "3           0_1_2_0_0_1_0_0_0_0_0_0_0_0_8_1_0_0_\n",
       "4           0_2_0_1_0_1_0_0_0_0_0_0_0_0_9_1_0_0_\n",
       "                           ...                  \n",
       "1488023     0_1_6_0_0_0_1_0_0_0_0_0_0_0_2_0_0_1_\n",
       "1488024    5_3_5_1_0_0_0_1_0_0_0_0_0_0_11_1_0_0_\n",
       "1488025     0_1_5_0_0_1_0_0_0_0_0_0_0_0_5_0_0_1_\n",
       "1488026    6_1_5_1_0_0_0_0_1_0_0_0_0_0_13_1_0_0_\n",
       "1488027    7_1_4_1_0_0_0_0_1_0_0_0_0_0_12_1_0_0_\n",
       "Name: mix_ind, Length: 1488028, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모든 ind 변수들의 값을 _로 연결한 mix_ind 생성 \n",
    "ind_features = [feature for feature in all_features\n",
    "                if 'ind' in feature] \n",
    "print(ind_features)\n",
    "\n",
    "is_first_feature = True \n",
    "for ind_feature in ind_features : \n",
    "    if is_first_feature : \n",
    "        all_data['mix_ind'] = all_data[ind_feature].astype(str) + '_'\n",
    "        is_first_feature = False\n",
    "    else : \n",
    "        all_data['mix_ind'] += all_data[ind_feature].astype(str) + '_'\n",
    "\n",
    "\n",
    "all_data['mix_ind']       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ps_ind_02_cat_count',\n",
       " 'ps_ind_04_cat_count',\n",
       " 'ps_ind_05_cat_count',\n",
       " 'ps_car_01_cat_count',\n",
       " 'ps_car_02_cat_count',\n",
       " 'ps_car_03_cat_count',\n",
       " 'ps_car_04_cat_count',\n",
       " 'ps_car_05_cat_count',\n",
       " 'ps_car_06_cat_count',\n",
       " 'ps_car_07_cat_count',\n",
       " 'ps_car_08_cat_count',\n",
       " 'ps_car_09_cat_count',\n",
       " 'ps_car_10_cat_count',\n",
       " 'ps_car_11_cat_count',\n",
       " 'mix_ind_count']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#명목형 변수들(cat, mix_ind)의 고유값별 갯수를 파생변수로 만들기 \n",
    "cat_count_features = []\n",
    "for feature in cat_features+['mix_ind'] :\n",
    "    val_counts_dict = all_data[feature].value_counts().to_dict()\n",
    "    all_data[f'{feature}_count'] = all_data[feature].apply(lambda x : val_counts_dict[x])\n",
    "    cat_count_features.append(f'{feature}_count')\n",
    "\n",
    "cat_count_features    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 생성된 변수들 \n",
    "  1) encoded_cat_matrix : 원핫 인코딩된 명목변수 \n",
    "  2) remaining_features : 명목형과 calc분류를 제외한 변수들 (num_missing 변수 추가)\n",
    "  3) cat_count_featues : max_ind를 포함한 명목평 변수들의 고유값별 갯수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불필요한 변수들 제거하고, 데이터 합치기 \n",
    "from scipy import sparse \n",
    "\n",
    "drop_features = ['ps_ind_14', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_car_14']\n",
    "all_data_remaining = all_data[remaining_features + cat_count_features].drop(drop_features, axis=1)\n",
    "\n",
    "all_data_sprs = sparse.hstack([sparse.csr_matrix(all_data_remaining), \n",
    "                               encoded_cat_matrix], format = 'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1488028x217 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 61125924 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_sprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(595212, 217) (892816, 217) (595212,)\n"
     ]
    }
   ],
   "source": [
    "#다시 train/test 데이터 분리 \n",
    "num_train = len(train)\n",
    "\n",
    "X = all_data_sprs[:num_train]\n",
    "X_test = all_data_sprs[num_train:]\n",
    "y = train['target'].values\n",
    "\n",
    "print(X.shape, X_test.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 2. Hyper parameter tunning : Bayesian optimizing \n",
    " - Grid search 보다 속도와 성능, 직관적인 코드 구성이 장점 \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesin을 위한 훈련 및 검증용 데이터셋 구성 \n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "bayes_dtrain = lgb.Dataset(X_train, y_train)\n",
    "bayes_dvalid = lgb.Dataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파라미터 범위 \n",
    "param_bounds = {'num_leaves': (30,40), \n",
    "                'lambda_l1':(0.7, 0.9), \n",
    "                'lambda_l2':(0.9, 1),\n",
    "                'feature_fraction': (0.6, 0.7), \n",
    "                'bagging_fraction': (0.6, 0.9), \n",
    "                'min_child_samples': (6, 10), \n",
    "                'min_child_weight': (10, 40) }\n",
    "\n",
    "#고정값\n",
    "fixed_params = {'objective': 'binary', \n",
    "                'learning_rate': 0.005,    ##보통 0.01~0.001 사이에서 지정, 속도를 위해 임의 지정함 \n",
    "                'bagging_freq': 1,         ##배깅빈도 1  \n",
    "                'force_row_wise': True, \n",
    "                'random_state' : 1991 \n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM용 gini() 함수 : 신백균 제공: https://www.kaggle.com/code/werooring/ch8-lgb-modeling\n",
    "def gini(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini', eval_gini(labels, preds), True # 반환값\n",
    "\n",
    "def eval_gini(y_true, y_pred):\n",
    "    # 실제값과 예측값의 크기가 같은지 확인 (값이 다르면 오류 발생)\n",
    "    assert y_true.shape == y_pred.shape\n",
    "\n",
    "    n_samples = y_true.shape[0]                      # 데이터 개수\n",
    "    L_mid = np.linspace(1 / n_samples, 1, n_samples) # 대각선 값\n",
    "\n",
    "    # 1) 예측값에 대한 지니계수\n",
    "    pred_order = y_true[y_pred.argsort()] # y_pred 크기순으로 y_true 값 정렬\n",
    "    L_pred = np.cumsum(pred_order) / np.sum(pred_order) # 로렌츠 곡선\n",
    "    G_pred = np.sum(L_mid - L_pred)       # 예측 값에 대한 지니계수\n",
    "\n",
    "    # 2) 예측이 완벽할 때 지니계수\n",
    "    true_order = y_true[y_true.argsort()] # y_true 크기순으로 y_true 값 정렬\n",
    "    L_true = np.cumsum(true_order) / np.sum(true_order) # 로렌츠 곡선\n",
    "    G_true = np.sum(L_mid - L_true)       # 예측이 완벽할 때 지니계수\n",
    "\n",
    "    # 정규화된 지니계수\n",
    "    return G_pred / G_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_function(num_leaves, lambda_l1, lambda_l2, feature_fraction,\n",
    "                  bagging_fraction, min_child_samples, min_child_weight):\n",
    "    '''최적화하려는 평가지표(지니계수) 계산 함수'''\n",
    "    \n",
    "    # 베이지안 최적화를 수행할 하이퍼파라미터 \n",
    "    params = {'num_leaves': int(round(num_leaves)),   ##최대 가지수\n",
    "              'lambda_l1': lambda_l1,                 ##정규화 강도\n",
    "              'lambda_l2': lambda_l2,\n",
    "              'feature_fraction': feature_fraction,   ##각 트리마다 사용될 피처의 비율. 일종의 피처 샘플링을 제어하는 매개변수\n",
    "              'bagging_fraction': bagging_fraction,   ##부스팅 과정에서 각 반복마다 사용될 데이터 샘플링 비율\n",
    "              'min_child_samples': int(round(min_child_samples)),   ##리프 노드가 되기 위한 최소 데이터 샘플 수입니다. 정수형 값으로 반올림하여 설정\n",
    "              'min_child_weight': min_child_weight,                 ##리프 노드가 되기 위한 최소 가중치 합. 일반적으로 데이터 샘플 가중치의 합을 제한하기 위해 사용\n",
    "              'feature_pre_filter': False}                          ##LightGBM 모델에서 사용할 피처의 사전 필터링을 사용할지 여부를 결정하는 매개변수\n",
    "    # 고정된 하이퍼파라미터도 추가\n",
    "    params.update(fixed_params)\n",
    "    \n",
    "    print('하이퍼파라미터:', params)    \n",
    "    \n",
    "    # LightGBM 모델 훈련\n",
    "    lgb_model = lgb.train(params=params, \n",
    "                          train_set=bayes_dtrain,\n",
    "                          num_boost_round=2500,\n",
    "                          valid_sets=bayes_dvalid,\n",
    "                          feval=gini,\n",
    "                          callbacks=[\n",
    "                             lgb.early_stopping(stopping_rounds=300),\n",
    "                             lgb.log_evaluation(period=100)  #100번째 반복마다 로그 출력\n",
    "                             ])\n",
    "\n",
    "\n",
    "    # 검증 데이터로 예측 수행\n",
    "    preds = lgb_model.predict(X_valid) \n",
    "    # 지니계수 계산\n",
    "    gini_score = eval_gini(y_valid, preds)\n",
    "    print(f'지니계수 : {gini_score}\\n')\n",
    "    \n",
    "    return gini_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#베이지안 최적화 객체 생성 \n",
    "#!pip install bayesian-optimization\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "optimizer = BayesianOptimization(f=eval_function, \n",
    "                                 pbounds = param_bounds,\n",
    "                                 random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | lambda_l1 | lambda_l2 | min_ch... | min_ch... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "하이퍼파라미터: {'num_leaves': 34, 'lambda_l1': 0.8205526752143287, 'lambda_l2': 0.9544883182996897, 'feature_fraction': 0.6715189366372419, 'bagging_fraction': 0.7646440511781974, 'min_child_samples': 8, 'min_child_weight': 29.376823391999682, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153606\tvalid_0's gini: 0.253552\n",
      "[200]\tvalid_0's binary_logloss: 0.152603\tvalid_0's gini: 0.260382\n",
      "[300]\tvalid_0's binary_logloss: 0.152048\tvalid_0's gini: 0.265475\n",
      "[400]\tvalid_0's binary_logloss: 0.151712\tvalid_0's gini: 0.269364\n",
      "[500]\tvalid_0's binary_logloss: 0.15149\tvalid_0's gini: 0.272577\n",
      "[600]\tvalid_0's binary_logloss: 0.15135\tvalid_0's gini: 0.27491\n",
      "[700]\tvalid_0's binary_logloss: 0.151256\tvalid_0's gini: 0.276626\n",
      "[800]\tvalid_0's binary_logloss: 0.151181\tvalid_0's gini: 0.278123\n",
      "[900]\tvalid_0's binary_logloss: 0.151124\tvalid_0's gini: 0.279563\n",
      "[1000]\tvalid_0's binary_logloss: 0.151085\tvalid_0's gini: 0.280511\n",
      "[1100]\tvalid_0's binary_logloss: 0.151051\tvalid_0's gini: 0.281479\n",
      "[1200]\tvalid_0's binary_logloss: 0.151031\tvalid_0's gini: 0.282071\n",
      "[1300]\tvalid_0's binary_logloss: 0.151013\tvalid_0's gini: 0.282616\n",
      "[1400]\tvalid_0's binary_logloss: 0.150997\tvalid_0's gini: 0.283196\n",
      "[1500]\tvalid_0's binary_logloss: 0.150984\tvalid_0's gini: 0.283672\n",
      "[1600]\tvalid_0's binary_logloss: 0.150978\tvalid_0's gini: 0.28402\n",
      "[1700]\tvalid_0's binary_logloss: 0.150968\tvalid_0's gini: 0.284403\n",
      "[1800]\tvalid_0's binary_logloss: 0.150964\tvalid_0's gini: 0.284582\n",
      "[1900]\tvalid_0's binary_logloss: 0.15096\tvalid_0's gini: 0.284764\n",
      "[2000]\tvalid_0's binary_logloss: 0.150956\tvalid_0's gini: 0.284981\n",
      "[2100]\tvalid_0's binary_logloss: 0.150953\tvalid_0's gini: 0.285194\n",
      "[2200]\tvalid_0's binary_logloss: 0.150951\tvalid_0's gini: 0.285328\n",
      "[2300]\tvalid_0's binary_logloss: 0.150951\tvalid_0's gini: 0.285443\n",
      "[2400]\tvalid_0's binary_logloss: 0.150952\tvalid_0's gini: 0.285399\n",
      "[2500]\tvalid_0's binary_logloss: 0.150952\tvalid_0's gini: 0.285547\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2345]\tvalid_0's binary_logloss: 0.150947\tvalid_0's gini: 0.285581\n",
      "지니계수 : 0.2855811556220905\n",
      "\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.2856   \u001b[0m | \u001b[0m0.7646   \u001b[0m | \u001b[0m0.6715   \u001b[0m | \u001b[0m0.8206   \u001b[0m | \u001b[0m0.9545   \u001b[0m | \u001b[0m7.695    \u001b[0m | \u001b[0m29.38    \u001b[0m | \u001b[0m34.38    \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 39, 'lambda_l1': 0.7766883037651555, 'lambda_l2': 0.9791725038082665, 'feature_fraction': 0.6963662760501029, 'bagging_fraction': 0.867531900234624, 'min_child_samples': 8, 'min_child_weight': 27.04133683281797, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153574\tvalid_0's gini: 0.252607\n",
      "[200]\tvalid_0's binary_logloss: 0.15257\tvalid_0's gini: 0.260296\n",
      "[300]\tvalid_0's binary_logloss: 0.152021\tvalid_0's gini: 0.265689\n",
      "[400]\tvalid_0's binary_logloss: 0.151692\tvalid_0's gini: 0.269611\n",
      "[500]\tvalid_0's binary_logloss: 0.151481\tvalid_0's gini: 0.272613\n",
      "[600]\tvalid_0's binary_logloss: 0.151344\tvalid_0's gini: 0.274815\n",
      "[700]\tvalid_0's binary_logloss: 0.15125\tvalid_0's gini: 0.276505\n",
      "[800]\tvalid_0's binary_logloss: 0.151176\tvalid_0's gini: 0.278175\n",
      "[900]\tvalid_0's binary_logloss: 0.151122\tvalid_0's gini: 0.279411\n",
      "[1000]\tvalid_0's binary_logloss: 0.151086\tvalid_0's gini: 0.28028\n",
      "[1100]\tvalid_0's binary_logloss: 0.151062\tvalid_0's gini: 0.280983\n",
      "[1200]\tvalid_0's binary_logloss: 0.151039\tvalid_0's gini: 0.281656\n",
      "[1300]\tvalid_0's binary_logloss: 0.151028\tvalid_0's gini: 0.282065\n",
      "[1400]\tvalid_0's binary_logloss: 0.151019\tvalid_0's gini: 0.282278\n",
      "[1500]\tvalid_0's binary_logloss: 0.151005\tvalid_0's gini: 0.28274\n",
      "[1600]\tvalid_0's binary_logloss: 0.151001\tvalid_0's gini: 0.282942\n",
      "[1700]\tvalid_0's binary_logloss: 0.151\tvalid_0's gini: 0.282975\n",
      "[1800]\tvalid_0's binary_logloss: 0.150993\tvalid_0's gini: 0.28324\n",
      "[1900]\tvalid_0's binary_logloss: 0.150991\tvalid_0's gini: 0.283264\n",
      "[2000]\tvalid_0's binary_logloss: 0.150989\tvalid_0's gini: 0.283434\n",
      "[2100]\tvalid_0's binary_logloss: 0.150985\tvalid_0's gini: 0.283657\n",
      "[2200]\tvalid_0's binary_logloss: 0.15099\tvalid_0's gini: 0.283575\n",
      "[2300]\tvalid_0's binary_logloss: 0.150994\tvalid_0's gini: 0.283624\n",
      "[2400]\tvalid_0's binary_logloss: 0.150997\tvalid_0's gini: 0.283636\n",
      "Early stopping, best iteration is:\n",
      "[2152]\tvalid_0's binary_logloss: 0.150984\tvalid_0's gini: 0.283738\n",
      "지니계수 : 0.2837380537005777\n",
      "\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.2837   \u001b[0m | \u001b[0m0.8675   \u001b[0m | \u001b[0m0.6964   \u001b[0m | \u001b[0m0.7767   \u001b[0m | \u001b[0m0.9792   \u001b[0m | \u001b[0m8.116    \u001b[0m | \u001b[0m27.04    \u001b[0m | \u001b[0m39.26    \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 40, 'lambda_l1': 0.7040436794880651, 'lambda_l2': 0.9832619845547939, 'feature_fraction': 0.608712929970154, 'bagging_fraction': 0.6213108174593661, 'min_child_samples': 9, 'min_child_weight': 36.10036444740457, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153583\tvalid_0's gini: 0.258806\n",
      "[200]\tvalid_0's binary_logloss: 0.152551\tvalid_0's gini: 0.264799\n",
      "[300]\tvalid_0's binary_logloss: 0.152002\tvalid_0's gini: 0.268244\n",
      "[400]\tvalid_0's binary_logloss: 0.151672\tvalid_0's gini: 0.271717\n",
      "[500]\tvalid_0's binary_logloss: 0.151461\tvalid_0's gini: 0.274397\n",
      "[600]\tvalid_0's binary_logloss: 0.151332\tvalid_0's gini: 0.276294\n",
      "[700]\tvalid_0's binary_logloss: 0.151236\tvalid_0's gini: 0.278096\n",
      "[800]\tvalid_0's binary_logloss: 0.151161\tvalid_0's gini: 0.279597\n",
      "[900]\tvalid_0's binary_logloss: 0.151105\tvalid_0's gini: 0.280953\n",
      "[1000]\tvalid_0's binary_logloss: 0.151061\tvalid_0's gini: 0.282166\n",
      "[1100]\tvalid_0's binary_logloss: 0.151028\tvalid_0's gini: 0.28302\n",
      "[1200]\tvalid_0's binary_logloss: 0.151003\tvalid_0's gini: 0.283738\n",
      "[1300]\tvalid_0's binary_logloss: 0.150989\tvalid_0's gini: 0.284264\n",
      "[1400]\tvalid_0's binary_logloss: 0.150974\tvalid_0's gini: 0.28468\n",
      "[1500]\tvalid_0's binary_logloss: 0.15097\tvalid_0's gini: 0.284846\n",
      "[1600]\tvalid_0's binary_logloss: 0.150963\tvalid_0's gini: 0.285159\n",
      "[1700]\tvalid_0's binary_logloss: 0.150959\tvalid_0's gini: 0.285335\n",
      "[1800]\tvalid_0's binary_logloss: 0.15096\tvalid_0's gini: 0.285354\n",
      "[1900]\tvalid_0's binary_logloss: 0.150963\tvalid_0's gini: 0.28532\n",
      "[2000]\tvalid_0's binary_logloss: 0.150957\tvalid_0's gini: 0.28551\n",
      "[2100]\tvalid_0's binary_logloss: 0.150954\tvalid_0's gini: 0.285718\n",
      "[2200]\tvalid_0's binary_logloss: 0.150956\tvalid_0's gini: 0.285753\n",
      "[2300]\tvalid_0's binary_logloss: 0.150959\tvalid_0's gini: 0.28577\n",
      "[2400]\tvalid_0's binary_logloss: 0.150961\tvalid_0's gini: 0.285801\n",
      "Early stopping, best iteration is:\n",
      "[2115]\tvalid_0's binary_logloss: 0.150953\tvalid_0's gini: 0.285785\n",
      "지니계수 : 0.2857848354322048\n",
      "\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m0.2858   \u001b[0m | \u001b[95m0.6213   \u001b[0m | \u001b[95m0.6087   \u001b[0m | \u001b[95m0.704    \u001b[0m | \u001b[95m0.9833   \u001b[0m | \u001b[95m9.113    \u001b[0m | \u001b[95m36.1     \u001b[0m | \u001b[95m39.79    \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 30, 'lambda_l1': 0.8444997594874222, 'lambda_l2': 0.9234023852202012, 'feature_fraction': 0.6593983245038058, 'bagging_fraction': 0.8977977822397395, 'min_child_samples': 9, 'min_child_weight': 10.549362495448534, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153656\tvalid_0's gini: 0.250788\n",
      "[200]\tvalid_0's binary_logloss: 0.152669\tvalid_0's gini: 0.257969\n",
      "[300]\tvalid_0's binary_logloss: 0.152107\tvalid_0's gini: 0.264083\n",
      "[400]\tvalid_0's binary_logloss: 0.15177\tvalid_0's gini: 0.26809\n",
      "[500]\tvalid_0's binary_logloss: 0.151553\tvalid_0's gini: 0.270998\n",
      "[600]\tvalid_0's binary_logloss: 0.15141\tvalid_0's gini: 0.27319\n",
      "[700]\tvalid_0's binary_logloss: 0.15131\tvalid_0's gini: 0.274828\n",
      "[800]\tvalid_0's binary_logloss: 0.151236\tvalid_0's gini: 0.276289\n",
      "[900]\tvalid_0's binary_logloss: 0.15118\tvalid_0's gini: 0.27752\n",
      "[1000]\tvalid_0's binary_logloss: 0.151141\tvalid_0's gini: 0.278541\n",
      "[1100]\tvalid_0's binary_logloss: 0.151119\tvalid_0's gini: 0.279195\n",
      "[1200]\tvalid_0's binary_logloss: 0.151091\tvalid_0's gini: 0.279956\n",
      "[1300]\tvalid_0's binary_logloss: 0.151077\tvalid_0's gini: 0.28049\n",
      "[1400]\tvalid_0's binary_logloss: 0.151063\tvalid_0's gini: 0.281001\n",
      "[1500]\tvalid_0's binary_logloss: 0.151047\tvalid_0's gini: 0.281425\n",
      "[1600]\tvalid_0's binary_logloss: 0.15104\tvalid_0's gini: 0.281733\n",
      "[1700]\tvalid_0's binary_logloss: 0.15103\tvalid_0's gini: 0.282035\n",
      "[1800]\tvalid_0's binary_logloss: 0.151027\tvalid_0's gini: 0.282124\n",
      "[1900]\tvalid_0's binary_logloss: 0.151024\tvalid_0's gini: 0.282286\n",
      "[2000]\tvalid_0's binary_logloss: 0.151016\tvalid_0's gini: 0.282509\n",
      "[2100]\tvalid_0's binary_logloss: 0.151013\tvalid_0's gini: 0.282659\n",
      "[2200]\tvalid_0's binary_logloss: 0.15101\tvalid_0's gini: 0.282753\n",
      "[2300]\tvalid_0's binary_logloss: 0.15101\tvalid_0's gini: 0.282795\n",
      "[2400]\tvalid_0's binary_logloss: 0.151009\tvalid_0's gini: 0.282926\n",
      "[2500]\tvalid_0's binary_logloss: 0.151009\tvalid_0's gini: 0.283033\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2328]\tvalid_0's binary_logloss: 0.151007\tvalid_0's gini: 0.282899\n",
      "지니계수 : 0.2828993761731121\n",
      "\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.2829   \u001b[0m | \u001b[0m0.8978   \u001b[0m | \u001b[0m0.6594   \u001b[0m | \u001b[0m0.8445   \u001b[0m | \u001b[0m0.9234   \u001b[0m | \u001b[0m8.619    \u001b[0m | \u001b[0m10.55    \u001b[0m | \u001b[0m30.09    \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 37, 'lambda_l1': 0.7738449330497988, 'lambda_l2': 0.9032695189818599, 'feature_fraction': 0.6606341064409726, 'bagging_fraction': 0.7666713964943057, 'min_child_samples': 9, 'min_child_weight': 29.306172421380474, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.15359\tvalid_0's gini: 0.254989\n",
      "[200]\tvalid_0's binary_logloss: 0.152586\tvalid_0's gini: 0.261328\n",
      "[300]\tvalid_0's binary_logloss: 0.152035\tvalid_0's gini: 0.265825\n",
      "[400]\tvalid_0's binary_logloss: 0.151705\tvalid_0's gini: 0.269579\n",
      "[500]\tvalid_0's binary_logloss: 0.151482\tvalid_0's gini: 0.272713\n",
      "[600]\tvalid_0's binary_logloss: 0.151341\tvalid_0's gini: 0.275062\n",
      "[700]\tvalid_0's binary_logloss: 0.151248\tvalid_0's gini: 0.276757\n",
      "[800]\tvalid_0's binary_logloss: 0.151173\tvalid_0's gini: 0.278228\n",
      "[900]\tvalid_0's binary_logloss: 0.151117\tvalid_0's gini: 0.279683\n",
      "[1000]\tvalid_0's binary_logloss: 0.151082\tvalid_0's gini: 0.280541\n",
      "[1100]\tvalid_0's binary_logloss: 0.151048\tvalid_0's gini: 0.281432\n",
      "[1200]\tvalid_0's binary_logloss: 0.151022\tvalid_0's gini: 0.282205\n",
      "[1300]\tvalid_0's binary_logloss: 0.151003\tvalid_0's gini: 0.282808\n",
      "[1400]\tvalid_0's binary_logloss: 0.150988\tvalid_0's gini: 0.283303\n",
      "[1500]\tvalid_0's binary_logloss: 0.150976\tvalid_0's gini: 0.283768\n",
      "[1600]\tvalid_0's binary_logloss: 0.15097\tvalid_0's gini: 0.284078\n",
      "[1700]\tvalid_0's binary_logloss: 0.150965\tvalid_0's gini: 0.284348\n",
      "[1800]\tvalid_0's binary_logloss: 0.15096\tvalid_0's gini: 0.284591\n",
      "[1900]\tvalid_0's binary_logloss: 0.150955\tvalid_0's gini: 0.284809\n",
      "[2000]\tvalid_0's binary_logloss: 0.150951\tvalid_0's gini: 0.284923\n",
      "[2100]\tvalid_0's binary_logloss: 0.150946\tvalid_0's gini: 0.285172\n",
      "[2200]\tvalid_0's binary_logloss: 0.150952\tvalid_0's gini: 0.285126\n",
      "[2300]\tvalid_0's binary_logloss: 0.150951\tvalid_0's gini: 0.285258\n",
      "Early stopping, best iteration is:\n",
      "[2044]\tvalid_0's binary_logloss: 0.150944\tvalid_0's gini: 0.285133\n",
      "지니계수 : 0.28513273331754563\n",
      "\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.2851   \u001b[0m | \u001b[0m0.7667   \u001b[0m | \u001b[0m0.6606   \u001b[0m | \u001b[0m0.7738   \u001b[0m | \u001b[0m0.9033   \u001b[0m | \u001b[0m8.769    \u001b[0m | \u001b[0m29.31    \u001b[0m | \u001b[0m36.6     \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 33, 'lambda_l1': 0.8268138957114485, 'lambda_l2': 0.9, 'feature_fraction': 0.6126817698209742, 'bagging_fraction': 0.6626247133728974, 'min_child_samples': 10, 'min_child_weight': 35.87746477712849, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153629\tvalid_0's gini: 0.256336\n",
      "[200]\tvalid_0's binary_logloss: 0.152624\tvalid_0's gini: 0.26217\n",
      "[300]\tvalid_0's binary_logloss: 0.152068\tvalid_0's gini: 0.266273\n",
      "[400]\tvalid_0's binary_logloss: 0.151725\tvalid_0's gini: 0.270202\n",
      "[500]\tvalid_0's binary_logloss: 0.151501\tvalid_0's gini: 0.27333\n",
      "[600]\tvalid_0's binary_logloss: 0.151359\tvalid_0's gini: 0.275446\n",
      "[700]\tvalid_0's binary_logloss: 0.151255\tvalid_0's gini: 0.277265\n",
      "[800]\tvalid_0's binary_logloss: 0.151174\tvalid_0's gini: 0.278969\n",
      "[900]\tvalid_0's binary_logloss: 0.151112\tvalid_0's gini: 0.280475\n",
      "[1000]\tvalid_0's binary_logloss: 0.151067\tvalid_0's gini: 0.281576\n",
      "[1100]\tvalid_0's binary_logloss: 0.151028\tvalid_0's gini: 0.282502\n",
      "[1200]\tvalid_0's binary_logloss: 0.151\tvalid_0's gini: 0.283229\n",
      "[1300]\tvalid_0's binary_logloss: 0.150982\tvalid_0's gini: 0.283737\n",
      "[1400]\tvalid_0's binary_logloss: 0.150968\tvalid_0's gini: 0.284145\n",
      "[1500]\tvalid_0's binary_logloss: 0.150952\tvalid_0's gini: 0.284688\n",
      "[1600]\tvalid_0's binary_logloss: 0.150942\tvalid_0's gini: 0.285031\n",
      "[1700]\tvalid_0's binary_logloss: 0.150941\tvalid_0's gini: 0.28515\n",
      "[1800]\tvalid_0's binary_logloss: 0.150935\tvalid_0's gini: 0.285406\n",
      "[1900]\tvalid_0's binary_logloss: 0.150937\tvalid_0's gini: 0.285396\n",
      "[2000]\tvalid_0's binary_logloss: 0.150933\tvalid_0's gini: 0.285594\n",
      "[2100]\tvalid_0's binary_logloss: 0.150934\tvalid_0's gini: 0.285642\n",
      "[2200]\tvalid_0's binary_logloss: 0.150936\tvalid_0's gini: 0.285646\n",
      "[2300]\tvalid_0's binary_logloss: 0.150935\tvalid_0's gini: 0.285742\n",
      "[2400]\tvalid_0's binary_logloss: 0.150936\tvalid_0's gini: 0.285735\n",
      "[2500]\tvalid_0's binary_logloss: 0.150942\tvalid_0's gini: 0.285637\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2328]\tvalid_0's binary_logloss: 0.150933\tvalid_0's gini: 0.285832\n",
      "지니계수 : 0.2858318138927059\n",
      "\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m0.2858   \u001b[0m | \u001b[95m0.6626   \u001b[0m | \u001b[95m0.6127   \u001b[0m | \u001b[95m0.8268   \u001b[0m | \u001b[95m0.9      \u001b[0m | \u001b[95m9.762    \u001b[0m | \u001b[95m35.88    \u001b[0m | \u001b[95m32.8     \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 36, 'lambda_l1': 0.7, 'lambda_l2': 0.9, 'feature_fraction': 0.7, 'bagging_fraction': 0.6, 'min_child_samples': 6, 'min_child_weight': 40.0, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153577\tvalid_0's gini: 0.256179\n",
      "[200]\tvalid_0's binary_logloss: 0.152559\tvalid_0's gini: 0.262322\n",
      "[300]\tvalid_0's binary_logloss: 0.152008\tvalid_0's gini: 0.266801\n",
      "[400]\tvalid_0's binary_logloss: 0.151681\tvalid_0's gini: 0.270446\n",
      "[500]\tvalid_0's binary_logloss: 0.151469\tvalid_0's gini: 0.273559\n",
      "[600]\tvalid_0's binary_logloss: 0.151331\tvalid_0's gini: 0.275887\n",
      "[700]\tvalid_0's binary_logloss: 0.151242\tvalid_0's gini: 0.277507\n",
      "[800]\tvalid_0's binary_logloss: 0.15117\tvalid_0's gini: 0.278984\n",
      "[900]\tvalid_0's binary_logloss: 0.151111\tvalid_0's gini: 0.280343\n",
      "[1000]\tvalid_0's binary_logloss: 0.151065\tvalid_0's gini: 0.2815\n",
      "[1100]\tvalid_0's binary_logloss: 0.151034\tvalid_0's gini: 0.282404\n",
      "[1200]\tvalid_0's binary_logloss: 0.151011\tvalid_0's gini: 0.283012\n",
      "[1300]\tvalid_0's binary_logloss: 0.150993\tvalid_0's gini: 0.283546\n",
      "[1400]\tvalid_0's binary_logloss: 0.15098\tvalid_0's gini: 0.283858\n",
      "[1500]\tvalid_0's binary_logloss: 0.150969\tvalid_0's gini: 0.284158\n",
      "[1600]\tvalid_0's binary_logloss: 0.150958\tvalid_0's gini: 0.28462\n",
      "[1700]\tvalid_0's binary_logloss: 0.150955\tvalid_0's gini: 0.284817\n",
      "[1800]\tvalid_0's binary_logloss: 0.150959\tvalid_0's gini: 0.28479\n",
      "[1900]\tvalid_0's binary_logloss: 0.150959\tvalid_0's gini: 0.2848\n",
      "[2000]\tvalid_0's binary_logloss: 0.150956\tvalid_0's gini: 0.285027\n",
      "Early stopping, best iteration is:\n",
      "[1722]\tvalid_0's binary_logloss: 0.150953\tvalid_0's gini: 0.284861\n",
      "지니계수 : 0.2848613395033384\n",
      "\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.2849   \u001b[0m | \u001b[0m0.6      \u001b[0m | \u001b[0m0.7      \u001b[0m | \u001b[0m0.7      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m6.0      \u001b[0m | \u001b[0m40.0     \u001b[0m | \u001b[0m35.9     \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 30, 'lambda_l1': 0.7, 'lambda_l2': 0.9, 'feature_fraction': 0.6, 'bagging_fraction': 0.6, 'min_child_samples': 10, 'min_child_weight': 30.859861883442534, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153668\tvalid_0's gini: 0.256071\n",
      "[200]\tvalid_0's binary_logloss: 0.15266\tvalid_0's gini: 0.261876\n",
      "[300]\tvalid_0's binary_logloss: 0.152107\tvalid_0's gini: 0.265685\n",
      "[400]\tvalid_0's binary_logloss: 0.15176\tvalid_0's gini: 0.269464\n",
      "[500]\tvalid_0's binary_logloss: 0.151543\tvalid_0's gini: 0.272211\n",
      "[600]\tvalid_0's binary_logloss: 0.151405\tvalid_0's gini: 0.274259\n",
      "[700]\tvalid_0's binary_logloss: 0.151307\tvalid_0's gini: 0.275869\n",
      "[800]\tvalid_0's binary_logloss: 0.151233\tvalid_0's gini: 0.277313\n",
      "[900]\tvalid_0's binary_logloss: 0.151166\tvalid_0's gini: 0.278857\n",
      "[1000]\tvalid_0's binary_logloss: 0.151125\tvalid_0's gini: 0.279794\n",
      "[1100]\tvalid_0's binary_logloss: 0.151088\tvalid_0's gini: 0.28084\n",
      "[1200]\tvalid_0's binary_logloss: 0.151053\tvalid_0's gini: 0.281848\n",
      "[1300]\tvalid_0's binary_logloss: 0.151036\tvalid_0's gini: 0.282363\n",
      "[1400]\tvalid_0's binary_logloss: 0.151015\tvalid_0's gini: 0.282852\n",
      "[1500]\tvalid_0's binary_logloss: 0.151006\tvalid_0's gini: 0.283115\n",
      "[1600]\tvalid_0's binary_logloss: 0.150995\tvalid_0's gini: 0.283536\n",
      "[1700]\tvalid_0's binary_logloss: 0.150988\tvalid_0's gini: 0.283828\n",
      "[1800]\tvalid_0's binary_logloss: 0.150984\tvalid_0's gini: 0.284032\n",
      "[1900]\tvalid_0's binary_logloss: 0.15098\tvalid_0's gini: 0.284216\n",
      "[2000]\tvalid_0's binary_logloss: 0.150975\tvalid_0's gini: 0.284419\n",
      "[2100]\tvalid_0's binary_logloss: 0.150971\tvalid_0's gini: 0.284541\n",
      "[2200]\tvalid_0's binary_logloss: 0.150972\tvalid_0's gini: 0.284588\n",
      "[2300]\tvalid_0's binary_logloss: 0.150969\tvalid_0's gini: 0.284784\n",
      "[2400]\tvalid_0's binary_logloss: 0.15097\tvalid_0's gini: 0.284848\n",
      "[2500]\tvalid_0's binary_logloss: 0.150966\tvalid_0's gini: 0.284986\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2477]\tvalid_0's binary_logloss: 0.150966\tvalid_0's gini: 0.284953\n",
      "지니계수 : 0.2849533150638962\n",
      "\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.285    \u001b[0m | \u001b[0m0.6      \u001b[0m | \u001b[0m0.6      \u001b[0m | \u001b[0m0.7      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m30.86    \u001b[0m | \u001b[0m30.0     \u001b[0m |\n",
      "하이퍼파라미터: {'num_leaves': 35, 'lambda_l1': 0.9, 'lambda_l2': 1.0, 'feature_fraction': 0.7, 'bagging_fraction': 0.9, 'min_child_samples': 6, 'min_child_weight': 33.74253635491814, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153613\tvalid_0's gini: 0.25072\n",
      "[200]\tvalid_0's binary_logloss: 0.152621\tvalid_0's gini: 0.258732\n",
      "[300]\tvalid_0's binary_logloss: 0.152064\tvalid_0's gini: 0.264625\n",
      "[400]\tvalid_0's binary_logloss: 0.151723\tvalid_0's gini: 0.269007\n",
      "[500]\tvalid_0's binary_logloss: 0.151503\tvalid_0's gini: 0.27228\n",
      "[600]\tvalid_0's binary_logloss: 0.151362\tvalid_0's gini: 0.274655\n",
      "[700]\tvalid_0's binary_logloss: 0.151261\tvalid_0's gini: 0.276548\n",
      "[800]\tvalid_0's binary_logloss: 0.151187\tvalid_0's gini: 0.27803\n",
      "[900]\tvalid_0's binary_logloss: 0.151137\tvalid_0's gini: 0.279265\n",
      "[1000]\tvalid_0's binary_logloss: 0.151099\tvalid_0's gini: 0.280271\n",
      "[1100]\tvalid_0's binary_logloss: 0.151069\tvalid_0's gini: 0.281038\n",
      "[1200]\tvalid_0's binary_logloss: 0.151037\tvalid_0's gini: 0.281842\n",
      "[1300]\tvalid_0's binary_logloss: 0.151024\tvalid_0's gini: 0.282296\n",
      "[1400]\tvalid_0's binary_logloss: 0.151011\tvalid_0's gini: 0.282688\n",
      "[1500]\tvalid_0's binary_logloss: 0.150998\tvalid_0's gini: 0.283076\n",
      "[1600]\tvalid_0's binary_logloss: 0.15099\tvalid_0's gini: 0.283308\n",
      "[1700]\tvalid_0's binary_logloss: 0.15098\tvalid_0's gini: 0.283613\n",
      "[1800]\tvalid_0's binary_logloss: 0.150973\tvalid_0's gini: 0.283904\n",
      "[1900]\tvalid_0's binary_logloss: 0.15097\tvalid_0's gini: 0.284057\n",
      "[2000]\tvalid_0's binary_logloss: 0.150969\tvalid_0's gini: 0.28416\n",
      "[2100]\tvalid_0's binary_logloss: 0.150968\tvalid_0's gini: 0.284338\n",
      "[2200]\tvalid_0's binary_logloss: 0.150968\tvalid_0's gini: 0.284468\n",
      "[2300]\tvalid_0's binary_logloss: 0.15097\tvalid_0's gini: 0.284457\n",
      "Early stopping, best iteration is:\n",
      "[2048]\tvalid_0's binary_logloss: 0.150966\tvalid_0's gini: 0.284294\n",
      "지니계수 : 0.28429416092349136\n",
      "\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.2843   \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m0.7      \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m6.0      \u001b[0m | \u001b[0m33.74    \u001b[0m | \u001b[0m35.4     \u001b[0m |\n",
      "=============================================================================================================\n",
      "코드실행 시간: 816.618325471878 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "#베이지안 최적화 실행 \n",
    "optimizer.maximize(init_points = 3, n_iter=6)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"코드실행 시간: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.6626247133728974,\n",
       " 'feature_fraction': 0.6126817698209742,\n",
       " 'lambda_l1': 0.8268138957114485,\n",
       " 'lambda_l2': 0.9,\n",
       " 'min_child_samples': 9.761611657760122,\n",
       " 'min_child_weight': 35.87746477712849,\n",
       " 'num_leaves': 32.800739086810694}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#optimized parameters \n",
    "max_params = optimizer.max['params']\n",
    "max_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.6626247133728974,\n",
       " 'feature_fraction': 0.6126817698209742,\n",
       " 'lambda_l1': 0.8268138957114485,\n",
       " 'lambda_l2': 0.9,\n",
       " 'min_child_samples': 10,\n",
       " 'min_child_weight': 35.87746477712849,\n",
       " 'num_leaves': 33,\n",
       " 'objective': 'binary',\n",
       " 'learning_rate': 0.005,\n",
       " 'bagging_freq': 1,\n",
       " 'force_row_wise': True,\n",
       " 'random_state': 1991}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_leaves, min_child_samples는 정수이므로 정수변환 \n",
    "max_params['num_leaves'] = int(round(max_params['num_leaves']))\n",
    "max_params['min_child_samples'] = int(round(max_params['min_child_samples']))\n",
    "\n",
    "# fixed parameters add \n",
    "max_params.update(fixed_params)\n",
    "\n",
    "#최종 파라미터 출력 \n",
    "max_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 최종 파라미터로 모델훈련 및 검증 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## fold 1 / fold 5 ##################################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1554\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154293\tvalid_0's gini: 0.268452\n",
      "[200]\tvalid_0's binary_logloss: 0.153249\tvalid_0's gini: 0.273856\n",
      "[300]\tvalid_0's binary_logloss: 0.15265\tvalid_0's gini: 0.278185\n",
      "[400]\tvalid_0's binary_logloss: 0.15229\tvalid_0's gini: 0.281623\n",
      "[500]\tvalid_0's binary_logloss: 0.152051\tvalid_0's gini: 0.284844\n",
      "[600]\tvalid_0's binary_logloss: 0.151891\tvalid_0's gini: 0.287294\n",
      "[700]\tvalid_0's binary_logloss: 0.151783\tvalid_0's gini: 0.289035\n",
      "[800]\tvalid_0's binary_logloss: 0.151694\tvalid_0's gini: 0.290689\n",
      "[900]\tvalid_0's binary_logloss: 0.151618\tvalid_0's gini: 0.292327\n",
      "[1000]\tvalid_0's binary_logloss: 0.151566\tvalid_0's gini: 0.293693\n",
      "[1100]\tvalid_0's binary_logloss: 0.151525\tvalid_0's gini: 0.294583\n",
      "[1200]\tvalid_0's binary_logloss: 0.151488\tvalid_0's gini: 0.295494\n",
      "[1300]\tvalid_0's binary_logloss: 0.151459\tvalid_0's gini: 0.29626\n",
      "[1400]\tvalid_0's binary_logloss: 0.151438\tvalid_0's gini: 0.296802\n",
      "[1500]\tvalid_0's binary_logloss: 0.151426\tvalid_0's gini: 0.296991\n",
      "[1600]\tvalid_0's binary_logloss: 0.15141\tvalid_0's gini: 0.297306\n",
      "[1700]\tvalid_0's binary_logloss: 0.15141\tvalid_0's gini: 0.297208\n",
      "[1800]\tvalid_0's binary_logloss: 0.151401\tvalid_0's gini: 0.297456\n",
      "[1900]\tvalid_0's binary_logloss: 0.15139\tvalid_0's gini: 0.297751\n",
      "[2000]\tvalid_0's binary_logloss: 0.151387\tvalid_0's gini: 0.297776\n",
      "[2100]\tvalid_0's binary_logloss: 0.151384\tvalid_0's gini: 0.297888\n",
      "[2200]\tvalid_0's binary_logloss: 0.151381\tvalid_0's gini: 0.298026\n",
      "[2300]\tvalid_0's binary_logloss: 0.15138\tvalid_0's gini: 0.298014\n",
      "[2400]\tvalid_0's binary_logloss: 0.151374\tvalid_0's gini: 0.298198\n",
      "[2500]\tvalid_0's binary_logloss: 0.151371\tvalid_0's gini: 0.298231\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2457]\tvalid_0's binary_logloss: 0.151368\tvalid_0's gini: 0.298367\n",
      "fold 1 gini_score: 0.29836704494920346 \n",
      "\n",
      "################################################## fold 2 / fold 5 ##################################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1560\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154393\tvalid_0's gini: 0.257016\n",
      "[200]\tvalid_0's binary_logloss: 0.153392\tvalid_0's gini: 0.26287\n",
      "[300]\tvalid_0's binary_logloss: 0.152849\tvalid_0's gini: 0.266917\n",
      "[400]\tvalid_0's binary_logloss: 0.152527\tvalid_0's gini: 0.270209\n",
      "[500]\tvalid_0's binary_logloss: 0.152328\tvalid_0's gini: 0.272743\n",
      "[600]\tvalid_0's binary_logloss: 0.152182\tvalid_0's gini: 0.275328\n",
      "[700]\tvalid_0's binary_logloss: 0.152089\tvalid_0's gini: 0.276901\n",
      "[800]\tvalid_0's binary_logloss: 0.152013\tvalid_0's gini: 0.278521\n",
      "[900]\tvalid_0's binary_logloss: 0.15196\tvalid_0's gini: 0.279835\n",
      "[1000]\tvalid_0's binary_logloss: 0.151918\tvalid_0's gini: 0.280879\n",
      "[1100]\tvalid_0's binary_logloss: 0.151882\tvalid_0's gini: 0.281856\n",
      "[1200]\tvalid_0's binary_logloss: 0.15185\tvalid_0's gini: 0.28271\n",
      "[1300]\tvalid_0's binary_logloss: 0.151823\tvalid_0's gini: 0.283434\n",
      "[1400]\tvalid_0's binary_logloss: 0.151805\tvalid_0's gini: 0.283875\n",
      "[1500]\tvalid_0's binary_logloss: 0.151788\tvalid_0's gini: 0.284323\n",
      "[1600]\tvalid_0's binary_logloss: 0.151777\tvalid_0's gini: 0.284722\n",
      "[1700]\tvalid_0's binary_logloss: 0.151773\tvalid_0's gini: 0.284765\n",
      "[1800]\tvalid_0's binary_logloss: 0.151767\tvalid_0's gini: 0.284916\n",
      "[1900]\tvalid_0's binary_logloss: 0.151763\tvalid_0's gini: 0.285044\n",
      "[2000]\tvalid_0's binary_logloss: 0.151753\tvalid_0's gini: 0.285295\n",
      "[2100]\tvalid_0's binary_logloss: 0.151749\tvalid_0's gini: 0.285452\n",
      "[2200]\tvalid_0's binary_logloss: 0.151745\tvalid_0's gini: 0.2855\n",
      "[2300]\tvalid_0's binary_logloss: 0.151744\tvalid_0's gini: 0.285535\n",
      "[2400]\tvalid_0's binary_logloss: 0.151742\tvalid_0's gini: 0.285515\n",
      "[2500]\tvalid_0's binary_logloss: 0.151743\tvalid_0's gini: 0.285512\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2443]\tvalid_0's binary_logloss: 0.151739\tvalid_0's gini: 0.285591\n",
      "fold 2 gini_score: 0.2855914620293369 \n",
      "\n",
      "################################################## fold 3 / fold 5 ##################################################\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1556\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154298\tvalid_0's gini: 0.260672\n",
      "[200]\tvalid_0's binary_logloss: 0.153236\tvalid_0's gini: 0.267048\n",
      "[300]\tvalid_0's binary_logloss: 0.152649\tvalid_0's gini: 0.270746\n",
      "[400]\tvalid_0's binary_logloss: 0.152287\tvalid_0's gini: 0.274296\n",
      "[500]\tvalid_0's binary_logloss: 0.15205\tvalid_0's gini: 0.277166\n",
      "[600]\tvalid_0's binary_logloss: 0.15189\tvalid_0's gini: 0.279428\n",
      "[700]\tvalid_0's binary_logloss: 0.15179\tvalid_0's gini: 0.280832\n",
      "[800]\tvalid_0's binary_logloss: 0.151714\tvalid_0's gini: 0.282248\n",
      "[900]\tvalid_0's binary_logloss: 0.151664\tvalid_0's gini: 0.283053\n",
      "[1000]\tvalid_0's binary_logloss: 0.151621\tvalid_0's gini: 0.283958\n",
      "[1100]\tvalid_0's binary_logloss: 0.151598\tvalid_0's gini: 0.284396\n",
      "[1200]\tvalid_0's binary_logloss: 0.151579\tvalid_0's gini: 0.28484\n",
      "[1300]\tvalid_0's binary_logloss: 0.151569\tvalid_0's gini: 0.285047\n",
      "[1400]\tvalid_0's binary_logloss: 0.15156\tvalid_0's gini: 0.285178\n",
      "[1500]\tvalid_0's binary_logloss: 0.151554\tvalid_0's gini: 0.285333\n",
      "[1600]\tvalid_0's binary_logloss: 0.151552\tvalid_0's gini: 0.285264\n",
      "[1700]\tvalid_0's binary_logloss: 0.151548\tvalid_0's gini: 0.285365\n",
      "[1800]\tvalid_0's binary_logloss: 0.151552\tvalid_0's gini: 0.285261\n",
      "[1900]\tvalid_0's binary_logloss: 0.151552\tvalid_0's gini: 0.285303\n",
      "Early stopping, best iteration is:\n",
      "[1677]\tvalid_0's binary_logloss: 0.151545\tvalid_0's gini: 0.28547\n",
      "fold 3 gini_score: 0.2854701452664806 \n",
      "\n",
      "################################################## fold 4 / fold 5 ##################################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154384\tvalid_0's gini: 0.254991\n",
      "[200]\tvalid_0's binary_logloss: 0.153366\tvalid_0's gini: 0.260595\n",
      "[300]\tvalid_0's binary_logloss: 0.152812\tvalid_0's gini: 0.26459\n",
      "[400]\tvalid_0's binary_logloss: 0.15249\tvalid_0's gini: 0.267601\n",
      "[500]\tvalid_0's binary_logloss: 0.15229\tvalid_0's gini: 0.270299\n",
      "[600]\tvalid_0's binary_logloss: 0.152149\tvalid_0's gini: 0.272508\n",
      "[700]\tvalid_0's binary_logloss: 0.152055\tvalid_0's gini: 0.274201\n",
      "[800]\tvalid_0's binary_logloss: 0.151986\tvalid_0's gini: 0.275558\n",
      "[900]\tvalid_0's binary_logloss: 0.151938\tvalid_0's gini: 0.27664\n",
      "[1000]\tvalid_0's binary_logloss: 0.151901\tvalid_0's gini: 0.27752\n",
      "[1100]\tvalid_0's binary_logloss: 0.151882\tvalid_0's gini: 0.277912\n",
      "[1200]\tvalid_0's binary_logloss: 0.151869\tvalid_0's gini: 0.278311\n",
      "[1300]\tvalid_0's binary_logloss: 0.151848\tvalid_0's gini: 0.278812\n",
      "[1400]\tvalid_0's binary_logloss: 0.151835\tvalid_0's gini: 0.279164\n",
      "[1500]\tvalid_0's binary_logloss: 0.15183\tvalid_0's gini: 0.279359\n",
      "[1600]\tvalid_0's binary_logloss: 0.151822\tvalid_0's gini: 0.27961\n",
      "[1700]\tvalid_0's binary_logloss: 0.151824\tvalid_0's gini: 0.279558\n",
      "[1800]\tvalid_0's binary_logloss: 0.151818\tvalid_0's gini: 0.279808\n",
      "[1900]\tvalid_0's binary_logloss: 0.151819\tvalid_0's gini: 0.279875\n",
      "[2000]\tvalid_0's binary_logloss: 0.151817\tvalid_0's gini: 0.280009\n",
      "[2100]\tvalid_0's binary_logloss: 0.151823\tvalid_0's gini: 0.27989\n",
      "Early stopping, best iteration is:\n",
      "[1869]\tvalid_0's binary_logloss: 0.151816\tvalid_0's gini: 0.279957\n",
      "fold 4 gini_score: 0.2799569943371714 \n",
      "\n",
      "################################################## fold 5 / fold 5 ##################################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1556\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 216\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154433\tvalid_0's gini: 0.264932\n",
      "[200]\tvalid_0's binary_logloss: 0.153444\tvalid_0's gini: 0.26984\n",
      "[300]\tvalid_0's binary_logloss: 0.152897\tvalid_0's gini: 0.273326\n",
      "[400]\tvalid_0's binary_logloss: 0.152558\tvalid_0's gini: 0.276928\n",
      "[500]\tvalid_0's binary_logloss: 0.152341\tvalid_0's gini: 0.280057\n",
      "[600]\tvalid_0's binary_logloss: 0.152183\tvalid_0's gini: 0.282865\n",
      "[700]\tvalid_0's binary_logloss: 0.152072\tvalid_0's gini: 0.285094\n",
      "[800]\tvalid_0's binary_logloss: 0.151981\tvalid_0's gini: 0.287129\n",
      "[900]\tvalid_0's binary_logloss: 0.151915\tvalid_0's gini: 0.288804\n",
      "[1000]\tvalid_0's binary_logloss: 0.151854\tvalid_0's gini: 0.290425\n",
      "[1100]\tvalid_0's binary_logloss: 0.151814\tvalid_0's gini: 0.291473\n",
      "[1200]\tvalid_0's binary_logloss: 0.151784\tvalid_0's gini: 0.29228\n",
      "[1300]\tvalid_0's binary_logloss: 0.151764\tvalid_0's gini: 0.292869\n",
      "[1400]\tvalid_0's binary_logloss: 0.151746\tvalid_0's gini: 0.293316\n",
      "[1500]\tvalid_0's binary_logloss: 0.151731\tvalid_0's gini: 0.293738\n",
      "[1600]\tvalid_0's binary_logloss: 0.151722\tvalid_0's gini: 0.293899\n",
      "[1700]\tvalid_0's binary_logloss: 0.151714\tvalid_0's gini: 0.294142\n",
      "[1800]\tvalid_0's binary_logloss: 0.151711\tvalid_0's gini: 0.294148\n",
      "[1900]\tvalid_0's binary_logloss: 0.151703\tvalid_0's gini: 0.294357\n",
      "[2000]\tvalid_0's binary_logloss: 0.151694\tvalid_0's gini: 0.294611\n",
      "[2100]\tvalid_0's binary_logloss: 0.15169\tvalid_0's gini: 0.294777\n",
      "[2200]\tvalid_0's binary_logloss: 0.15169\tvalid_0's gini: 0.29474\n",
      "[2300]\tvalid_0's binary_logloss: 0.15169\tvalid_0's gini: 0.294753\n",
      "[2400]\tvalid_0's binary_logloss: 0.15169\tvalid_0's gini: 0.294777\n",
      "[2500]\tvalid_0's binary_logloss: 0.15169\tvalid_0's gini: 0.294829\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2336]\tvalid_0's binary_logloss: 0.151686\tvalid_0's gini: 0.294901\n",
      "fold 5 gini_score: 0.2949013203165818 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#k-fold = 5\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)\n",
    "\n",
    "#훈련된 모델의 검증/테스트 타겟값을 담을 배열 \n",
    "oof_val_preds = np.zeros(X.shape[0])\n",
    "oof_test_preds = np.zeros(X_test.shape[0])\n",
    "\n",
    "#model train, predict, validation \n",
    "for idx, (train_idx, valid_idx) in enumerate(folds.split(X,y)): \n",
    "    print(\"#\"*50, f'fold {idx+1} / fold {folds.n_splits}', \"#\"*50)\n",
    "\n",
    "    X_train, y_train = X[train_idx], y[train_idx]    ##train data\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx]    ##valid data\n",
    "\n",
    "    dtrain = lgb.Dataset(X_train, y_train)           ##LightGBM용 train data\n",
    "    dvalid = lgb.Dataset(X_valid, y_valid)           ##LightGBM용 valid data\n",
    "\n",
    "    #LightGBM model training \n",
    "    lgb_model = lgb.train(params = max_params, \n",
    "                          train_set = dtrain, \n",
    "                          num_boost_round = 2500, \n",
    "                          valid_sets = dvalid, \n",
    "                          feval = gini, \n",
    "                          callbacks=[\n",
    "                             lgb.early_stopping(stopping_rounds=300),\n",
    "                             lgb.log_evaluation(period=100)  #100번째 반복마다 로그 출력\n",
    "                             ] )\n",
    "    \n",
    "    #모델의 예측값 \n",
    "    oof_test_preds += lgb_model.predict(X_test) / folds.n_splits\n",
    "    oof_val_preds[valid_idx] += lgb_model.predict(X_valid)\n",
    "\n",
    "    #정규화 gini score \n",
    "    gini_score = eval_gini(y_valid, oof_val_preds[valid_idx])\n",
    "    print(f'fold {idx+1} gini_score: {gini_score} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gini_score_valid =  0.2888026261834394\n",
      "gini_score_test =  [0.02680365 0.02346368 0.02330406 ... 0.03454174 0.02150611 0.02828947]\n"
     ]
    }
   ],
   "source": [
    "#결과 제출 \n",
    "print('gini_score_valid = ', eval_gini(y, oof_val_preds))\n",
    "print('gini_score_test = ', oof_test_preds)\n",
    "\n",
    "submission['target'] = oof_test_preds\n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/gini_2.PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
