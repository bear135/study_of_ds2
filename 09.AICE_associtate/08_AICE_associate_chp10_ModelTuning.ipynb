{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part1. 기본학습 - AI 핵심이론 및 활용     \n",
    "    chp10 - 모델 튜닝 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 대표적인 하이퍼 파라미터 자동튜닝 기법  \n",
    "- (1) GridSearchCV : 모든 파라미터 값의 조합에 대해 모델성능 비교 \n",
    "- (2) RandomizedSearchCV : 랜덤 조합이 선택, 파라미터 탐색범위가 넓거나, 연속적인 값을 탐색해야 하는 경우 효율적 \n",
    "- 사이킷런의 model_selection 서브 패키지에 포함  \n",
    "- 사용자가 튜닝하려는 하이퍼 파라미터 값들을 리스트로 지정 -> 딕셔너리 자료형 구성 -> (1) 또는 (2)의 클래스 객체 생성 -> estimator 객채 훈련/튜닝\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본데이터 형태 = torch.Size([6000, 28, 28]) torch.Size([6000]) torch.Size([1000, 28, 28]) torch.Size([1000])\n",
      "변경된 데이터 형태 = torch.Size([6000, 784]) torch.Size([1000, 784])\n"
     ]
    }
   ],
   "source": [
    "# MNIST 데이터셋 로드\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()  # 데이터를 텐서로 변환\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "# 학습 및 테스트 데이터를 분리\n",
    "x_train = train_data.data[:6000]\n",
    "y_train = train_data.targets[:6000]\n",
    "x_test = test_data.data[:1000]\n",
    "y_test = test_data.targets[:1000]\n",
    "\n",
    "print('원본데이터 형태 =', x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "# 이미지 데이터는 3차원 행렬 -> 2차원으로 변경 필요\n",
    "X_train = x_train.reshape(-1, 784)\n",
    "X_test = x_test.reshape(-1, 784)\n",
    "\n",
    "print('변경된 데이터 형태 =', X_train.shape, X_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forrest with GridSearchCV (Grid Search Cross Validation)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "estimator = RandomForestClassifier()\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, None],           # 트리의 최대 깊이의 경우의 수\n",
    "    'n_estimators': [10, 100, 200],          # 앙상블에 포함된 트리 갯수의 경우의 수\n",
    "    'max_features': [1, 2, 3],               # 각 트리에 사용되는 최대 특성 갯수\n",
    "    'min_samples_leaf': [1, 2, 3],           # 리프 노드(최말단 노드)에 포함될 샘플의 최소 갯수\n",
    "    'min_samples_split': [2, 5, 10]          # 트리가 분할되기 위한 최소 샘플 갯수 (1은 제거)\n",
    "}\n",
    "\n",
    "gs_cv = GridSearchCV(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "model = gs_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters =  {'max_depth': None, 'max_features': 3, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best score =  0.9313333333333333\n"
     ]
    }
   ],
   "source": [
    "## Best parameters & Model score\n",
    "print('Best parameters = ', model.best_params_)\n",
    "print('Best score = ', model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.924"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test data에 적용 후 모델성능 \n",
    "clf = model.best_estimator_\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint \n",
    "\n",
    "estimator = RandomForestClassifier()\n",
    "param_distributions = {'max_depth': list(np.arange(3,13,step=3)) + [None],    ## 트리의 최대 깊이(3, 6, 9, 12, 무제한)\n",
    "                       'n_estimators': np.arange(10,320,step=100),            ## 트리 갯수(10, 110, 210, 310)\n",
    "                       'max_features': randint(1,7),                          ## 각 트리에 사용할 최대 특성 갯수(랜덤으로 [1, 6] 범위 선택)\n",
    "                       'criterion': ['gini', 'entropy'],                      ## 트리 분할 기준(gini 또는 entropy)\n",
    "                       'min_samples_leaf': randint(1,4),                      ## 리프 노드(말단 노드)에 포함될 최소 샘플 갯수(랜덤으로 [1, 3] 범위 선택) \n",
    "                       'min_samples_split': np.arange(2,8, step=2)            ## 트리가 분할되기 위한 최소 샘플 갯수(2, 4, 6)\n",
    "                       }\n",
    "\n",
    "rs_cv = RandomizedSearchCV(estimator=estimator, \n",
    "                           param_distributions=param_distributions,           ## 하이퍼파라미터 범위\n",
    "                           n_iter = 10,                                       ## 랜덤으로 10개의 하이퍼파라미터 조합을 시도\n",
    "                           scoring='accuracy',                                ## 평가 기준을 정확도로 설정\n",
    "                           n_jobs = 1,                                        ## 1코어로 실행 \n",
    "                           refit=True,                                        ## best 하이퍼파라미터 조합으로 모델 재학습\n",
    "                           cv=3                                               ## 3-폴드 교차 검증\n",
    "                           )\n",
    "\n",
    "model = rs_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters =  {'criterion': 'entropy', 'max_depth': None, 'max_features': 5, 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 10}\n",
      "Best score =  0.8609999999999999\n",
      "Test score =  0.829\n"
     ]
    }
   ],
   "source": [
    "## Best parameters & Model score\n",
    "print('Best parameters = ', model.best_params_)\n",
    "print('Best score = ', model.best_score_)\n",
    "\n",
    "## test data에 적용 후 모델성능 \n",
    "clf = model.best_estimator_\n",
    "clf.fit(X_train, y_train)\n",
    "print('Test score = ', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 실습1. [회귀] 항공권 가격예측 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>flight</th>\n",
       "      <th>source_city</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>stops</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>destination_city</th>\n",
       "      <th>class</th>\n",
       "      <th>duration</th>\n",
       "      <th>days_left</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SpiceJet</td>\n",
       "      <td>SG-8709</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Evening</td>\n",
       "      <td>zero</td>\n",
       "      <td>Night</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1</td>\n",
       "      <td>5953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SpiceJet</td>\n",
       "      <td>SG-8157</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "      <td>5953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AirAsia</td>\n",
       "      <td>I5-764</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1</td>\n",
       "      <td>5956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>UK-995</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1</td>\n",
       "      <td>5955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>UK-963</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "      <td>5955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    airline   flight source_city departure_time stops   arrival_time  \\\n",
       "0  SpiceJet  SG-8709       Delhi        Evening  zero          Night   \n",
       "1  SpiceJet  SG-8157       Delhi  Early_Morning  zero        Morning   \n",
       "2   AirAsia   I5-764       Delhi  Early_Morning  zero  Early_Morning   \n",
       "3   Vistara   UK-995       Delhi        Morning  zero      Afternoon   \n",
       "4   Vistara   UK-963       Delhi        Morning  zero        Morning   \n",
       "\n",
       "  destination_city    class  duration  days_left  price  \n",
       "0           Mumbai  Economy      2.17          1   5953  \n",
       "1           Mumbai  Economy      2.33          1   5953  \n",
       "2           Mumbai  Economy      2.17          1   5956  \n",
       "3           Mumbai  Economy      2.25          1   5955  \n",
       "4           Mumbai  Economy      2.33          1   5955  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data \n",
    "cdf = pd.read_csv('data/Clean_Dataset.csv')\n",
    "cdf = cdf[:5000]\n",
    "cdf.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "cdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>flight</th>\n",
       "      <th>source_city</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>stops</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>destination_city</th>\n",
       "      <th>class</th>\n",
       "      <th>duration</th>\n",
       "      <th>days_left</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6</td>\n",
       "      <td>222</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>UK-819</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Evening</td>\n",
       "      <td>one</td>\n",
       "      <td>Night</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1496</td>\n",
       "      <td>90</td>\n",
       "      <td>5000</td>\n",
       "      <td>1391</td>\n",
       "      <td>3619</td>\n",
       "      <td>1702</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.665682</td>\n",
       "      <td>14.216800</td>\n",
       "      <td>7589.786600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.247512</td>\n",
       "      <td>7.109536</td>\n",
       "      <td>4476.362204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2409.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.330000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4678.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.670000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>5955.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.080000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10549.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.080000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>31260.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        airline  flight source_city departure_time stops arrival_time  \\\n",
       "count      5000    5000        5000           5000  5000         5000   \n",
       "unique        6     222           1              6     3            6   \n",
       "top     Vistara  UK-819       Delhi        Evening   one        Night   \n",
       "freq       1496      90        5000           1391  3619         1702   \n",
       "mean        NaN     NaN         NaN            NaN   NaN          NaN   \n",
       "std         NaN     NaN         NaN            NaN   NaN          NaN   \n",
       "min         NaN     NaN         NaN            NaN   NaN          NaN   \n",
       "25%         NaN     NaN         NaN            NaN   NaN          NaN   \n",
       "50%         NaN     NaN         NaN            NaN   NaN          NaN   \n",
       "75%         NaN     NaN         NaN            NaN   NaN          NaN   \n",
       "max         NaN     NaN         NaN            NaN   NaN          NaN   \n",
       "\n",
       "       destination_city    class     duration    days_left         price  \n",
       "count              5000     5000  5000.000000  5000.000000   5000.000000  \n",
       "unique                1        1          NaN          NaN           NaN  \n",
       "top              Mumbai  Economy          NaN          NaN           NaN  \n",
       "freq               5000     5000          NaN          NaN           NaN  \n",
       "mean                NaN      NaN     9.665682    14.216800   7589.786600  \n",
       "std                 NaN      NaN     7.247512     7.109536   4476.362204  \n",
       "min                 NaN      NaN     2.000000     1.000000   2409.000000  \n",
       "25%                 NaN      NaN     2.330000     8.000000   4678.000000  \n",
       "50%                 NaN      NaN     7.670000    14.000000   5955.000000  \n",
       "75%                 NaN      NaN    14.080000    20.000000  10549.000000  \n",
       "max                 NaN      NaN    30.080000    26.000000  31260.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "airline             0\n",
       "flight              0\n",
       "source_city         0\n",
       "departure_time      0\n",
       "stops               0\n",
       "arrival_time        0\n",
       "destination_city    0\n",
       "class               0\n",
       "duration            0\n",
       "days_left           0\n",
       "price               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(cdf.describe(include='all'))    ## 변수들 확인 \n",
    "cdf.isnull().sum()                      ## 결측치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='airline', ylabel='price'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# airline 변수를 모델에 포함시켜야 할까? (가격과 연관성이 있는지?)\n",
    "plt.figure(figsize=(5,2))\n",
    "sns.barplot(data=cdf, x='airline', y='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>source_city</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>stops</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>destination_city</th>\n",
       "      <th>class</th>\n",
       "      <th>duration</th>\n",
       "      <th>days_left</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SpiceJet</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Evening</td>\n",
       "      <td>zero</td>\n",
       "      <td>Night</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1</td>\n",
       "      <td>5953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SpiceJet</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "      <td>5953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    airline source_city departure_time stops arrival_time destination_city  \\\n",
       "0  SpiceJet       Delhi        Evening  zero        Night           Mumbai   \n",
       "1  SpiceJet       Delhi  Early_Morning  zero      Morning           Mumbai   \n",
       "\n",
       "     class  duration  days_left  price  \n",
       "0  Economy      2.17          1   5953  \n",
       "1  Economy      2.33          1   5953  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flight 변수를 모델에 포함시켜야 할까? (가격과 연관성이 있는지?)\n",
    "## 배경지식 : 편명(flight)는 각 항공사마다 국내선/국제선/사업지역에 따라 부여되는 코드체계 \n",
    "## 가격 예측과 별 상관없을 것으로 판단됨 \n",
    "cdf.drop('flight', axis=1, inplace=True)\n",
    "cdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>days_left</th>\n",
       "      <th>price</th>\n",
       "      <th>airline_Air_India</th>\n",
       "      <th>airline_GO_FIRST</th>\n",
       "      <th>airline_Indigo</th>\n",
       "      <th>airline_SpiceJet</th>\n",
       "      <th>airline_Vistara</th>\n",
       "      <th>departure_time_Early_Morning</th>\n",
       "      <th>departure_time_Evening</th>\n",
       "      <th>departure_time_Late_Night</th>\n",
       "      <th>departure_time_Morning</th>\n",
       "      <th>departure_time_Night</th>\n",
       "      <th>stops_two_or_more</th>\n",
       "      <th>stops_zero</th>\n",
       "      <th>arrival_time_Early_Morning</th>\n",
       "      <th>arrival_time_Evening</th>\n",
       "      <th>arrival_time_Late_Night</th>\n",
       "      <th>arrival_time_Morning</th>\n",
       "      <th>arrival_time_Night</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.17</td>\n",
       "      <td>1</td>\n",
       "      <td>5953</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "      <td>5953</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration  days_left  price  airline_Air_India  airline_GO_FIRST  \\\n",
       "0      2.17          1   5953              False             False   \n",
       "1      2.33          1   5953              False             False   \n",
       "\n",
       "   airline_Indigo  airline_SpiceJet  airline_Vistara  \\\n",
       "0           False              True            False   \n",
       "1           False              True            False   \n",
       "\n",
       "   departure_time_Early_Morning  departure_time_Evening  \\\n",
       "0                         False                    True   \n",
       "1                          True                   False   \n",
       "\n",
       "   departure_time_Late_Night  departure_time_Morning  departure_time_Night  \\\n",
       "0                      False                   False                 False   \n",
       "1                      False                   False                 False   \n",
       "\n",
       "   stops_two_or_more  stops_zero  arrival_time_Early_Morning  \\\n",
       "0              False        True                       False   \n",
       "1              False        True                       False   \n",
       "\n",
       "   arrival_time_Evening  arrival_time_Late_Night  arrival_time_Morning  \\\n",
       "0                 False                    False                 False   \n",
       "1                 False                    False                  True   \n",
       "\n",
       "   arrival_time_Night  \n",
       "0                True  \n",
       "1               False  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encoding \n",
    "dummies_cdf = pd.get_dummies(cdf, \n",
    "                             columns=['airline', 'source_city', 'departure_time', 'stops', 'arrival_time', 'destination_city', 'class'], \n",
    "                             drop_first=True\n",
    "                             )\n",
    "print(dummies_cdf.shape)\n",
    "dummies_cdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 19) (5000,)\n"
     ]
    }
   ],
   "source": [
    "# traing datasets \n",
    "y = dummies_cdf.price\n",
    "x = dummies_cdf.drop('price', axis=1)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500, 19) (3500,) (1500, 19) (1500,)\n"
     ]
    }
   ],
   "source": [
    "# 7 modeling \n",
    "#!pip install xgboost \n",
    "#!pip install lightgbm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "lr = LinearRegression()\n",
    "dtr = DecisionTreeRegressor(random_state=42)\n",
    "rfr = RandomForestRegressor(random_state=42)\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "etr = ExtraTreesRegressor(n_jobs=-1, random_state=42)\n",
    "xgbr = XGBRegressor(n_jobs=-1, random_state=42)\n",
    "lgbmr = LGBMRegressor(n_jobs=-1, random_state=42)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000636 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 3500, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7569.883429\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(n_jobs=-1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LGBMRegressor<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMRegressor(n_jobs=-1, random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training \n",
    "lr.fit(x_train, y_train)\n",
    "dtr.fit(x_train, y_train)\n",
    "rfr.fit(x_train, y_train)\n",
    "gbr.fit(x_train, y_train)\n",
    "etr.fit(x_train, y_train)\n",
    "xgbr.fit(x_train, y_train)\n",
    "lgbmr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>dtr</th>\n",
       "      <th>rfr</th>\n",
       "      <th>gbr</th>\n",
       "      <th>etr</th>\n",
       "      <th>xgbr</th>\n",
       "      <th>lgbmr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>0.630559</td>\n",
       "      <td>0.643057</td>\n",
       "      <td>0.796723</td>\n",
       "      <td>0.785569</td>\n",
       "      <td>0.714658</td>\n",
       "      <td>0.797469</td>\n",
       "      <td>0.808824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>2691.022061</td>\n",
       "      <td>2645.114267</td>\n",
       "      <td>1996.133032</td>\n",
       "      <td>2050.164963</td>\n",
       "      <td>2364.981237</td>\n",
       "      <td>1992.463855</td>\n",
       "      <td>1935.804688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               lr          dtr          rfr          gbr          etr  \\\n",
       "r2       0.630559     0.643057     0.796723     0.785569     0.714658   \n",
       "rmse  2691.022061  2645.114267  1996.133032  2050.164963  2364.981237   \n",
       "\n",
       "             xgbr        lgbmr  \n",
       "r2       0.797469     0.808824  \n",
       "rmse  1992.463855  1935.804688  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가 \n",
    "from sklearn.metrics import r2_score, root_mean_squared_error \n",
    "models = [lr, dtr, rfr, gbr, etr, xgbr, lgbmr]\n",
    "r2_score_list = []\n",
    "rmse_score_list = []\n",
    "\n",
    "for model in models: \n",
    "    pred = model.predict(x_test)\n",
    "    r2_score_list.append(r2_score(y_test, pred))\n",
    "    rmse_score_list.append(root_mean_squared_error(y_test, pred))\n",
    "\n",
    "r2_score_df = pd.DataFrame([r2_score_list, rmse_score_list], \n",
    "                           columns=['lr', 'dtr', 'rfr', 'gbr', 'etr', 'xgbr', 'lgbmr'], \n",
    "                           index=['r2', 'rmse']\n",
    "                           )\n",
    "r2_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000184 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000151 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000160 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000054 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000151 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000138 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000049 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000134 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000051 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000219 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000049 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000051 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000156 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000154 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000108 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 3500, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7569.883429\n",
      "beat parameters =  {'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 20}\n",
      "beat score =  0.8036925831129313\n"
     ]
    }
   ],
   "source": [
    "# best model인 lgbmr에 대해, 파라미터 튜닝 (GridSearchCV)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'learning_rate': [0.1, 0.01, 0.003], \n",
    "          'colsample_bytree':[0.5, 0.7], \n",
    "          'max_depth':[20,30,40] \n",
    "          }\n",
    "\n",
    "cv_lgbmr = GridSearchCV(estimator=lgbmr, \n",
    "                        param_grid=params,                         \n",
    "                        cv=5, verbose=1\n",
    "                        )\n",
    "\n",
    "cv_lgbmr.fit(x_train, y_train)\n",
    "\n",
    "print('beat parameters = ', cv_lgbmr.best_params_)\n",
    "print('beat score = ', cv_lgbmr.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000074 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 3500, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7569.883429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "r2_score =  0.8123365053453238\n",
      "rmse =  1917.9393759096415\n"
     ]
    }
   ],
   "source": [
    "# beat parameters 적용하여 모델 재실행 \n",
    "best_lgbmr = LGBMRegressor(max_depth=20, \n",
    "                           num_leaves=128,\n",
    "                           learning_rate=0.1, \n",
    "                           colsample_bytree=0.7, \n",
    "                           n_jobs=-1, \n",
    "                           random_state=42\n",
    "                           )\n",
    "\n",
    "best_lgbmr.fit(x_train, y_train)\n",
    "\n",
    "b_pred = best_lgbmr.predict(x_test)\n",
    "print('r2_score = ', r2_score(y_test, b_pred))\n",
    "print('rmse = ', root_mean_squared_error(y_test, b_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearchCV 적용결과 \n",
    "- 기존모델 vs. 최적모델 r2_score : 0.808824 -> 0.812336\n",
    "- 기존모델 vs. 최적모델 rmse : 1935.804688 -> 1917.939375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seoyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 18 is smaller than n_iter=500. Running 18 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000172 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000192 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000117 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000174 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000145 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000157 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000131 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000155 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000055 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000142 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7554.416429\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 224\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7558.646071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000054 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 226\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7533.341071\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7625.134286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 7577.879286\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 3500, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7569.883429\n",
      "beat parameters =  {'max_depth': 20, 'learning_rate': 0.1, 'colsample_bytree': 0.7}\n"
     ]
    }
   ],
   "source": [
    "# best model인 lgbmr에 대해, 파라미터 튜닝 (RandomizedSearchCV)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "params = {'learning_rate': [0.1, 0.01, 0.001], \n",
    "          'colsample_bytree':[0.5, 0.7], \n",
    "          'max_depth':[20,30,40] \n",
    "          }\n",
    "\n",
    "cv_lgbmr = RandomizedSearchCV(estimator=lgbmr, \n",
    "                              param_distributions=params, \n",
    "                              n_iter=500, \n",
    "                              cv=5, verbose=1\n",
    "                              )\n",
    "\n",
    "cv_lgbmr.fit(x_train, y_train)\n",
    "\n",
    "print('beat parameters = ', cv_lgbmr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000156 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 3500, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 7569.883429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "r2_score =  0.8123365053453238\n",
      "rmse =  1917.9393759096415\n"
     ]
    }
   ],
   "source": [
    "# beat parameters 적용하여 모델 재실행 \n",
    "best_lgbmr = LGBMRegressor(max_depth=20, \n",
    "                           num_leaves=128,\n",
    "                           learning_rate=0.1, \n",
    "                           colsample_bytree=0.7, \n",
    "                           n_jobs=-1, \n",
    "                           random_state=42\n",
    "                           )\n",
    "\n",
    "best_lgbmr.fit(x_train, y_train)\n",
    "\n",
    "b_pred = best_lgbmr.predict(x_test)\n",
    "print('r2_score = ', r2_score(y_test, b_pred))\n",
    "print('rmse = ', root_mean_squared_error(y_test, b_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomizedCV 최적화 적용 결과 \n",
    "- 기존모델 vs. 최적모델 r2_score : 0.808824 -> 0.812336\n",
    "- 기존모델 vs. 최적모델 rmse : 1935.804688 -> 1917.939375\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실습2. [분류] 항공사 고객만족 여부 예측 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Customer Type</th>\n",
       "      <th>Age</th>\n",
       "      <th>Type of Travel</th>\n",
       "      <th>Class</th>\n",
       "      <th>Flight Distance</th>\n",
       "      <th>Seat comfort</th>\n",
       "      <th>Departure/Arrival time convenient</th>\n",
       "      <th>Food and drink</th>\n",
       "      <th>Gate location</th>\n",
       "      <th>Inflight wifi service</th>\n",
       "      <th>Inflight entertainment</th>\n",
       "      <th>Online support</th>\n",
       "      <th>Ease of Online booking</th>\n",
       "      <th>On-board service</th>\n",
       "      <th>Leg room service</th>\n",
       "      <th>Baggage handling</th>\n",
       "      <th>Checkin service</th>\n",
       "      <th>Cleanliness</th>\n",
       "      <th>Online boarding</th>\n",
       "      <th>Departure Delay in Minutes</th>\n",
       "      <th>Arrival Delay in Minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>satisfied</td>\n",
       "      <td>Female</td>\n",
       "      <td>Loyal Customer</td>\n",
       "      <td>65</td>\n",
       "      <td>Personal Travel</td>\n",
       "      <td>Eco</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>satisfied</td>\n",
       "      <td>Male</td>\n",
       "      <td>Loyal Customer</td>\n",
       "      <td>47</td>\n",
       "      <td>Personal Travel</td>\n",
       "      <td>Business</td>\n",
       "      <td>2464</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>310</td>\n",
       "      <td>305.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  satisfaction  Gender   Customer Type  Age   Type of Travel     Class  \\\n",
       "0    satisfied  Female  Loyal Customer   65  Personal Travel       Eco   \n",
       "1    satisfied    Male  Loyal Customer   47  Personal Travel  Business   \n",
       "\n",
       "   Flight Distance  Seat comfort  Departure/Arrival time convenient  \\\n",
       "0              265             0                                  0   \n",
       "1             2464             0                                  0   \n",
       "\n",
       "   Food and drink  Gate location  Inflight wifi service  \\\n",
       "0               0              2                      2   \n",
       "1               0              3                      0   \n",
       "\n",
       "   Inflight entertainment  Online support  Ease of Online booking  \\\n",
       "0                       4               2                       3   \n",
       "1                       2               2                       3   \n",
       "\n",
       "   On-board service  Leg room service  Baggage handling  Checkin service  \\\n",
       "0                 3                 0                 3                5   \n",
       "1                 4                 4                 4                2   \n",
       "\n",
       "   Cleanliness  Online boarding  Departure Delay in Minutes  \\\n",
       "0            3                2                           0   \n",
       "1            3                2                         310   \n",
       "\n",
       "   Arrival Delay in Minutes  \n",
       "0                       0.0  \n",
       "1                     305.0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)          ## None : 모든 열을 출력, 숫자를 지정하면 그만큼의 열을 출력함  \n",
    "cdf = pd.read_csv('data/Invistico_Airline.csv')\n",
    "cdf = cdf[:5000]\n",
    "cdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "satisfaction\n",
      "satisfied       2869\n",
      "dissatisfied    2131\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "satisfaction\n",
      "satisfied       0.5738\n",
      "dissatisfied    0.4262\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# target = 'satisfaction' 분포확인 \n",
    "print(cdf['satisfaction'].value_counts())\n",
    "print('-'*100)\n",
    "print(cdf['satisfaction'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "satisfaction                          0\n",
      "Gender                                0\n",
      "Customer Type                         0\n",
      "Age                                   0\n",
      "Type of Travel                        0\n",
      "Class                                 0\n",
      "Flight Distance                       0\n",
      "Seat comfort                          0\n",
      "Departure/Arrival time convenient     0\n",
      "Food and drink                        0\n",
      "Gate location                         0\n",
      "Inflight wifi service                 0\n",
      "Inflight entertainment                0\n",
      "Online support                        0\n",
      "Ease of Online booking                0\n",
      "On-board service                      0\n",
      "Leg room service                      0\n",
      "Baggage handling                      0\n",
      "Checkin service                       0\n",
      "Cleanliness                           0\n",
      "Online boarding                       0\n",
      "Departure Delay in Minutes            0\n",
      "Arrival Delay in Minutes             27\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 결측치 확인 \n",
    "print(cdf.isnull().sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4973 entries, 0 to 4999\n",
      "Data columns (total 23 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   satisfaction                       4973 non-null   object \n",
      " 1   Gender                             4973 non-null   object \n",
      " 2   Customer Type                      4973 non-null   object \n",
      " 3   Age                                4973 non-null   int64  \n",
      " 4   Type of Travel                     4973 non-null   object \n",
      " 5   Class                              4973 non-null   object \n",
      " 6   Flight Distance                    4973 non-null   int64  \n",
      " 7   Seat comfort                       4973 non-null   int64  \n",
      " 8   Departure/Arrival time convenient  4973 non-null   int64  \n",
      " 9   Food and drink                     4973 non-null   int64  \n",
      " 10  Gate location                      4973 non-null   int64  \n",
      " 11  Inflight wifi service              4973 non-null   int64  \n",
      " 12  Inflight entertainment             4973 non-null   int64  \n",
      " 13  Online support                     4973 non-null   int64  \n",
      " 14  Ease of Online booking             4973 non-null   int64  \n",
      " 15  On-board service                   4973 non-null   int64  \n",
      " 16  Leg room service                   4973 non-null   int64  \n",
      " 17  Baggage handling                   4973 non-null   int64  \n",
      " 18  Checkin service                    4973 non-null   int64  \n",
      " 19  Cleanliness                        4973 non-null   int64  \n",
      " 20  Online boarding                    4973 non-null   int64  \n",
      " 21  Departure Delay in Minutes         4973 non-null   int64  \n",
      " 22  Arrival Delay in Minutes           4973 non-null   float64\n",
      "dtypes: float64(1), int64(17), object(5)\n",
      "memory usage: 932.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 결측치 = 27/5000로 매우 미미 --> 삭제 \n",
    "cdf.dropna(axis=0, inplace=True)\n",
    "display(cdf.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4973, 22) (4973,)\n"
     ]
    }
   ],
   "source": [
    "# datasets \n",
    "y = cdf['satisfaction']\n",
    "x = cdf.drop('satisfaction', axis=1)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Flight Distance</th>\n",
       "      <th>Seat comfort</th>\n",
       "      <th>Departure/Arrival time convenient</th>\n",
       "      <th>Food and drink</th>\n",
       "      <th>Gate location</th>\n",
       "      <th>Inflight wifi service</th>\n",
       "      <th>Inflight entertainment</th>\n",
       "      <th>Online support</th>\n",
       "      <th>Ease of Online booking</th>\n",
       "      <th>On-board service</th>\n",
       "      <th>Leg room service</th>\n",
       "      <th>Baggage handling</th>\n",
       "      <th>Checkin service</th>\n",
       "      <th>Cleanliness</th>\n",
       "      <th>Online boarding</th>\n",
       "      <th>Departure Delay in Minutes</th>\n",
       "      <th>Arrival Delay in Minutes</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Customer Type_Loyal Customer</th>\n",
       "      <th>Type of Travel_Personal Travel</th>\n",
       "      <th>Class_Business</th>\n",
       "      <th>Class_Eco</th>\n",
       "      <th>Class_Eco Plus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>2464</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>310</td>\n",
       "      <td>305.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Flight Distance  Seat comfort  Departure/Arrival time convenient  \\\n",
       "0   65              265             0                                  0   \n",
       "1   47             2464             0                                  0   \n",
       "\n",
       "   Food and drink  Gate location  Inflight wifi service  \\\n",
       "0               0              2                      2   \n",
       "1               0              3                      0   \n",
       "\n",
       "   Inflight entertainment  Online support  Ease of Online booking  \\\n",
       "0                       4               2                       3   \n",
       "1                       2               2                       3   \n",
       "\n",
       "   On-board service  Leg room service  Baggage handling  Checkin service  \\\n",
       "0                 3                 0                 3                5   \n",
       "1                 4                 4                 4                2   \n",
       "\n",
       "   Cleanliness  Online boarding  Departure Delay in Minutes  \\\n",
       "0            3                2                           0   \n",
       "1            3                2                         310   \n",
       "\n",
       "   Arrival Delay in Minutes  Gender_Female  Gender_Male  \\\n",
       "0                       0.0           True        False   \n",
       "1                     305.0          False         True   \n",
       "\n",
       "   Customer Type_Loyal Customer  Type of Travel_Personal Travel  \\\n",
       "0                          True                            True   \n",
       "1                          True                            True   \n",
       "\n",
       "   Class_Business  Class_Eco  Class_Eco Plus  \n",
       "0           False       True           False  \n",
       "1            True      False           False  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encoding \n",
    "x_gd = pd.get_dummies(x, \n",
    "                      columns=x.select_dtypes(include=['object']).columns, \n",
    "                      drop_first=False)\n",
    "x_gd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labed data :  [1 0 1 ... 1 0 1]\n",
      "labed class : ['dissatisfied' 'satisfied']\n"
     ]
    }
   ],
   "source": [
    "# labeling \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_gd, y, stratify=y, test_size=0.2, random_state=42)\n",
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(y_train)                      ## le가 y_train에 대해 학습 \n",
    "le_y_train = le.transform(y_train)   ## le가 y_train에 대해 학습한 결과로 y_train을 변환 \n",
    "le_y_test = le.transform(y_test)     ## le가 y_train에 대해 학습한 결과로 y_test을 변환  (le.fit_trasform(y_test)를 쓰면, y_test를 학습한 결과로 변환하므로 안됨)\n",
    "\n",
    "print('labed data : ', le_y_train)\n",
    "print('labed class :', le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seoyeon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 2284, number of negative: 1694\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000615 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 673\n",
      "[LightGBM] [Info] Number of data points in the train set: 3978, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.574158 -> initscore=0.298836\n",
      "[LightGBM] [Info] Start training from score 0.298836\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "학습모델 :  LogisticRegression()\n",
      "train 정확도:  0.89844\n",
      "test 정확도:  0.89146\n",
      "----------------------------------\n",
      "학습모델 :  DecisionTreeClassifier(random_state=1)\n",
      "train 정확도:  1.0\n",
      "test 정확도:  0.99899\n",
      "----------------------------------\n",
      "학습모델 :  RandomForestClassifier(random_state=1)\n",
      "train 정확도:  1.0\n",
      "test 정확도:  0.99899\n",
      "----------------------------------\n",
      "학습모델 :  GradientBoostingClassifier(random_state=1)\n",
      "train 정확도:  1.0\n",
      "test 정확도:  0.99899\n",
      "----------------------------------\n",
      "학습모델 :  XGBRFClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                colsample_bylevel=None, colsample_bytree=None, device=None,\n",
      "                early_stopping_rounds=None, enable_categorical=False,\n",
      "                eval_metric=None, feature_types=None, gamma=None,\n",
      "                grow_policy=None, importance_type=None,\n",
      "                interaction_constraints=None, max_bin=None,\n",
      "                max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "                max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "                min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "                multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "                num_parallel_tree=None, objective='binary:logistic',\n",
      "                random_state=1, reg_alpha=None, ...)\n",
      "train 정확도:  0.99975\n",
      "test 정확도:  0.99899\n",
      "----------------------------------\n",
      "학습모델 :  ExtraTreesClassifier(random_state=1)\n",
      "train 정확도:  1.0\n",
      "test 정확도:  0.99799\n",
      "----------------------------------\n",
      "학습모델 :  LGBMClassifier(random_state=1)\n",
      "train 정확도:  1.0\n",
      "test 정확도:  1.0\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# modeling \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from xgboost import XGBRFClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lr = LogisticRegression()\n",
    "dtc = DecisionTreeClassifier(random_state=1)\n",
    "rfc = RandomForestClassifier(random_state=1)\n",
    "gbc = GradientBoostingClassifier(random_state=1)\n",
    "xgbc = XGBRFClassifier(random_state=1)\n",
    "etc = ExtraTreesClassifier(random_state=1)\n",
    "lgbmc = LGBMClassifier(random_state=1)\n",
    "\n",
    "lr.fit(x_train, le_y_train)\n",
    "dtc.fit(x_train, le_y_train)\n",
    "rfc.fit(x_train, le_y_train)\n",
    "gbc.fit(x_train, le_y_train)\n",
    "xgbc.fit(x_train, le_y_train)\n",
    "etc.fit(x_train, le_y_train)\n",
    "lgbmc.fit(x_train, le_y_train)\n",
    "\n",
    "# 순서대로 적용할 모델을 리스트에 저장하기\n",
    "models = [lr, dtc, rfc, gbc, xgbc, etc, lgbmc]\n",
    "\n",
    "# for문을 활용해 학습 모델 별 Score를 리스트에 저장하기\n",
    "acc_train_list = []\n",
    "acc_test_list = []\n",
    "for model in models:\n",
    "    acc_train_list.append(round(model.score(x_train, le_y_train),5))\n",
    "    acc_test_list.append(round(model.score(x_test, le_y_test),5))\n",
    "\n",
    "# 모델 별 정확도를 출력하기\n",
    "for i in range(len(models)):\n",
    "    print('학습모델 : ',models[i])\n",
    "    print('train 정확도: ',acc_train_list[i])\n",
    "    print('test 정확도: ',acc_test_list[i])\n",
    "    print('----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터가 5000건으로 적어 대부분의 모델이 100%의 정확도를 달성함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적의 파라미터 학습시 Score : 0.99975\n",
      "최적의 파라미터 : {'max_depth': 10, 'max_features': None, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 최적화 (by GridSearchCV)\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "# param_grid를 정의하여 각 파라미터 별로 교차해서 모든 학습을 수행하기\n",
    "param_grid = { \n",
    "    'n_estimators': [50 ,100, 200, 500],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'max_depth' : [10,20,30,40,50,None],\n",
    "}\n",
    "\n",
    "# estimator=rfc :사용할 모델 지정\n",
    "# param_grid=param_grid : 미리 정의한 파라미터들을 교차 적용하기\n",
    "# n_jobs=-1 : -1로 지정하면 모든 CPU 활용하기\n",
    "cv_rfc = GridSearchCV(\n",
    "    estimator=rfc, \n",
    "    param_grid=param_grid, \n",
    "    n_jobs=-1,\n",
    "    cv=5)\n",
    "\n",
    "# 학습 수행하기\n",
    "cv_rfc.fit(x_train, le_y_train) \n",
    "\n",
    "# best_score를 출력하기\n",
    "print('최적의 파라미터 학습시 Score :',round(cv_rfc.best_score_ , 5)) \n",
    "\n",
    "# best params을 출력하기\n",
    "print('최적의 파라미터 :',cv_rfc.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적의 파라미터 학습시 Score : 0.99975\n",
      "최적의 파라미터 : {'n_estimators': 50, 'max_features': 'log2', 'max_depth': 30}\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 최적화 (by RandomizedSearchCV)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = { \n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'max_depth': [10, 20, 30, 40, 50, None],\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV 설정\n",
    "cv_rfc = RandomizedSearchCV(\n",
    "    estimator=rfc,            # 사용할 모델 지정\n",
    "    param_distributions=param_dist,  # 파라미터 분포 설정\n",
    "    n_iter=50,                # 테스트할 조합의 개수 (임의 샘플링 횟수)\n",
    "    n_jobs=-1,                # 모든 CPU 코어 활용\n",
    "    cv=5,                     # 교차 검증 폴드 수\n",
    "    random_state=42           # 재현성 확보를 위한 시드 설정\n",
    ")\n",
    "\n",
    "# 학습 수행\n",
    "cv_rfc.fit(x_train, le_y_train)\n",
    "\n",
    "# 최적의 점수 출력\n",
    "print('최적의 파라미터 학습시 Score :', round(cv_rfc.best_score_, 5))\n",
    "\n",
    "# 최적의 파라미터 출력\n",
    "print('최적의 파라미터 :', cv_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적의 파라미터 반영 Test Score : 0.99899\n"
     ]
    }
   ],
   "source": [
    "## 최적 파라미터를 적용한 모델 실행 \n",
    "rfc = RandomForestClassifier(n_estimators=50, \n",
    "                             max_features='log2', \n",
    "                             n_jobs = -1, \n",
    "                             max_depth=30, \n",
    "                             random_state=42\n",
    "                             )\n",
    "\n",
    "rfc.fit(x_train, le_y_train)\n",
    "print('최적의 파라미터 반영 Test Score :', round(rfc.score(x_test, le_y_test), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       424\n",
      "           1       1.00      1.00      1.00       571\n",
      "\n",
      "    accuracy                           1.00       995\n",
      "   macro avg       1.00      1.00      1.00       995\n",
      "weighted avg       1.00      1.00      1.00       995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## cf. confusion matrix \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# rfc 모델로 x_test를 예측한 값을 y_pred로 저장하기\n",
    "y_pred = rfc.predict(x_test) \n",
    "\n",
    "# 실제값 le_y_test(레이블 인코딩을 해야 0,1로 표현됨)와 예측값 y_pred 비교하기\n",
    "cm = confusion_matrix(le_y_test, y_pred, labels=[0,1])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
    "disp.plot()\n",
    "\n",
    "# 오차행렬(Confusion Matrix) 출력하기\n",
    "plt.show() \n",
    "\n",
    "# 오차행렬(Confusion Matrix)를 통한 각종 지표들을 리포트로 출력하기\n",
    "print(classification_report(le_y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
